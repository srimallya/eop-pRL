{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhXTMUhAcqwe8pSPbqgm0c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srimallya/eop-pRL/blob/main/eop_pRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End-Only Penalty Progressive Reinforcement Learning: A Mathematical Analysis\n",
        "\n",
        "**Abstract**\n",
        "\n",
        "This paper presents a comprehensive mathematical analysis of End-Only Penalty Progressive Reinforcement Learning (EOP-PRL), a novel reinforcement learning algorithm designed for training language models on structured reasoning tasks. Unlike traditional token-level reinforcement learning approaches, EOP-PRL employs a unique reward mechanism that exclusively penalizes incomplete generations rather than token-level discrepancies. We formalize the mathematical foundations of this approach, analyze its convergence properties, and demonstrate how it creates an effective balance between exploration and exploitation. Our analysis reveals that EOP-PRL introduces a natural curriculum through progressive penalty scaling, directly addressing the exploration-exploitation dilemma inherent in language model fine-tuning. We prove that under reasonable assumptions, EOP-PRL converges to policies that prioritize the generation of complete reasoning paths while allowing for greater diversity in intermediate token selection, a crucial requirement for complex reasoning tasks.\n",
        "\n",
        "**1. Introduction**\n",
        "\n",
        "Language models have demonstrated remarkable capabilities in various natural language processing tasks, yet they often struggle with structured reasoning that requires step-by-step logical thinking. Traditional supervised learning approaches using maximum likelihood estimation (MLE) tend to produce models that imitate the surface patterns in the training data without developing robust reasoning abilities [1]. For instance, in tasks requiring multi-step deduction, an MLE-trained model might learn to predict the next word based on the immediate context without truly understanding the underlying logical structure.\n",
        "\n",
        "Reinforcement Learning (RL) has emerged as a promising approach for fine-tuning language models beyond simple imitation. However, conventional RL methods for language models typically employ token-level reward structures that penalize any deviation from reference sequences [2, 3]. This approach can be counterproductive for reasoning tasks, where multiple valid reasoning paths may lead to the same conclusion. The strict token-level matching effectively restricts the model's ability to explore alternative reasoning approaches. Consider a mathematical proof where different intermediate steps can lead to the same final answer. A token-level penalty would discourage the model from exploring these alternative valid steps.\n",
        "\n",
        "We introduce End-Only Penalty Progressive Reinforcement Learning (EOP-PRL), a mathematical framework that fundamentally redefines the reward structure for training language models on reasoning tasks. The key innovation of EOP-PRL is its novel reward function that:\n",
        "\n",
        "* Rewards matching tokens with position-dependent scaling\n",
        "* Applies zero penalties for non-matching tokens during generation\n",
        "* Implements an end-only penalty exclusively for incomplete sequences\n",
        "* Incorporates a progressive penalty scaling that increases with training progress\n",
        "\n",
        "The main contributions of this paper are:\n",
        "\n",
        "* The introduction of the End-Only Penalty Progressive Reinforcement Learning (EOP-PRL) algorithm.\n",
        "* A rigorous mathematical formulation of the EOP-PRL reward function and policy gradient objective.\n",
        "* A theoretical analysis demonstrating how EOP-PRL balances exploration and exploitation.\n",
        "* A proof of convergence for EOP-PRL under standard policy gradient assumptions.\n",
        "* The identification of a natural curriculum learning effect arising from the progressive penalty scaling.\n",
        "* A theoretical comparison highlighting the advantages of EOP-PRL over traditional token-level penalty approaches in encouraging exploration.\n",
        "\n",
        "This paper provides a rigorous mathematical analysis of EOP-PRL, demonstrating how it resolves the exploration-exploitation trade-off in language model fine-tuning while ensuring convergence to policies that produce complete, structured reasoning.\n",
        "\n",
        "**2. Background and Related Work**\n",
        "\n",
        "**2.1 Policy Gradient Methods**\n",
        "\n",
        "Policy gradient methods are a family of reinforcement learning algorithms that directly optimize policy parameters by following the gradient of expected return [4]. For a policy $\\pi_\\theta$ parameterized by $\\theta$, the standard policy gradient objective is:\n",
        "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)] \\quad (1)$$\n",
        "where $\\tau$ represents a trajectory and $R(\\tau)$ is the return. The REINFORCE algorithm [5] provides an unbiased estimate of the gradient:\n",
        "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)] \\quad (2)$$\n",
        "In the context of language models, a trajectory corresponds to a sequence of tokens, and the policy represents the probability distribution over tokens at each generation step.\n",
        "\n",
        "**2.2 RL for Language Model Fine-tuning**\n",
        "\n",
        "Recent work has applied RL to language model fine-tuning, with prominent examples including RLHF (Reinforcement Learning from Human Feedback) [6], PPO (Proximal Policy Optimization) for language models [7], and various approaches using KL-regularized policy optimization [8]. These methods often rely on reward functions that evaluate the quality of the entire generated sequence based on human preferences or external metrics. Token-level rewards based on exact matching with reference sequences are also common. For instance, in sequence generation tasks, a reward might be given for each token that matches the ground truth, and a penalty for each mismatch. While effective for tasks like summarization or translation where the output is often unique, these approaches can restrict the model's ability to discover alternative, yet valid, reasoning steps in more complex tasks.\n",
        "\n",
        "**2.3 Curriculum Learning in RL**\n",
        "\n",
        "Curriculum learning [9] involves training models on increasingly difficult examples or with progressively higher standards. In reinforcement learning, curriculum learning has been implemented through environment complexity [10], reward shaping [11], and adversarial approaches [12]. EOP-PRL introduces a novel form of curriculum learning through its progressive penalty scaling. Initially, the penalty for incomplete sequences is relatively mild, allowing the model to focus on generating any reasonable output. As training progresses, the penalty increases, gradually pushing the model to generate complete and structured reasoning paths. This mirrors the idea of learning a complex skill by starting with simpler aspects and gradually increasing the difficulty.\n",
        "\n",
        "**3. Mathematical Formulation of EOP-PRL**\n",
        "\n",
        "**3.1 Notation and Preliminaries**\n",
        "\n",
        "We define the following notation:\n",
        "\n",
        "* $\\pi_\\theta$: Policy network (language model with parameters $\\theta$)\n",
        "* $x$: Input prompt\n",
        "* $y^*$: Reference output sequence\n",
        "* $\\hat{y}$: Generated output sequence\n",
        "* $\\hat{y}_t$: Token at position $t$ in the generated sequence\n",
        "* $r_t$: Reward for token at position $t$\n",
        "* $|\\hat{y}|$: Length of generated sequence\n",
        "* $|y^*|$: Length of reference sequence\n",
        "* $e$: Current episode number\n",
        "* $E$: Total number of episodes\n",
        "* $\\alpha$: Base reward value (default: 1.0)\n",
        "* $\\beta$: Base penalty value (default: -0.5)\n",
        "* $\\gamma$: Maximum penalty scaling factor (default: 2.5)\n",
        "\n",
        "**3.2 Reward Function**\n",
        "\n",
        "The core innovation of EOP-PRL lies in its novel reward function. For a token $\\hat{y}_t$ at position $t$ in the generated sequence $\\hat{y}$, the reward $r_t$ is defined as:\n",
        "\n",
        "* **Position-scaled reward for matching tokens:**\n",
        "    If $\\hat{y}_t = y^*_t$, then:\n",
        "    $$r_t = \\alpha \\cdot (0.1 + 0.9 \\cdot \\frac{t}{|\\hat{y}|}) \\quad (3)$$\n",
        "    This formulation provides several key properties:\n",
        "    * Rewards increase with position, prioritizing correct tokens later in the sequence. This encourages the model to maintain correctness as the reasoning path progresses. The scaling factor $(0.1 + 0.9 \\cdot \\frac{t}{|\\hat{y}|})$ ensures that later correct tokens contribute more significantly to the overall reward.\n",
        "    * The minimum reward (at $t = 0$) is $0.1\\alpha$, ensuring even early matches receive some reward. This prevents the model from completely disregarding initial correct steps.\n",
        "    * The maximum reward (at $t = |\\hat{y}|$) is $\\alpha$, normalizing the scale of positive rewards.\n",
        "\n",
        "* **Zero penalty for non-matching tokens during generation:**\n",
        "    If $\\hat{y}_t \\neq y^*_t$, then:\n",
        "    $$r_t = 0 \\quad (4)$$\n",
        "    Unlike conventional approaches that would apply a negative reward, this formulation allows exploration of alternative reasoning paths without penalty. The model is free to try different intermediate tokens without immediate negative feedback, fostering exploration of the solution space.\n",
        "\n",
        "* **End-only penalty for incomplete generations:**\n",
        "    If $|\\hat{y}| < |y^*|$ and $t = |\\hat{y}| - 1$ (the last generated token), then:\n",
        "    $$r_t += \\beta \\cdot S(e, E) \\cdot \\frac{|y^*| - |\\hat{y}|}{|y^*|} \\quad (5)$$\n",
        "    The end-only penalty is applied at the end of the generation process if the generated sequence length $|\\hat{y}|$ is less than the reference sequence length $|y^*|$. This component penalizes only incomplete sequences, with penalties proportional to:\n",
        "    * The degree of incompleteness: $\\frac{|y^*| - |\\hat{y}|}{|y^*|}$. The more incomplete the sequence, the larger the penalty.\n",
        "    * The training progress: $S(e, E)$ increases from 1.0 to $\\gamma$ over training, where $S(e, E)$ is the episode-dependent penalty scaling function:\n",
        "        $$S(e, E) = 1.0 + \\frac{e}{E-1} \\cdot (\\gamma - 1.0) \\quad (6)$$\n",
        "\n",
        "**3.3 Policy Gradient Objective**\n",
        "\n",
        "For training the policy $\\pi_\\theta$, we use the REINFORCE algorithm with the objective:\n",
        "$$J(\\theta) = -\\mathbb{E}_{\\hat{y} \\sim \\pi_\\theta(\\cdot|x)}[\\sum_{t=0}^{|\\hat{y}|-1} r_t \\cdot \\log \\pi_\\theta(\\hat{y}_t | x, \\hat{y}_{<t})] \\quad (7)$$\n",
        "The gradient is approximated using a single sampled sequence per step:\n",
        "$$\\nabla_\\theta J(\\theta) \\approx -\\sum_{t=0}^{|\\hat{y}|-1} r_t \\cdot \\nabla_\\theta \\log \\pi_\\theta(\\hat{y}_t | x, \\hat{y}_{<t}) \\quad (8)$$\n",
        "For implementation purposes, we can express this using a loss function $L$:\n",
        "$$L = -\\sum_{t=0}^{|\\hat{y}|-1} r_t \\cdot \\log \\pi_\\theta(\\hat{y}_t | x, \\hat{y}_{<t}) \\quad (9)$$\n",
        "When using gradient accumulation over $G$ steps, each step's loss is scaled by $\\frac{1}{G}$:\n",
        "$$L' = \\frac{L}{G} \\quad (10)$$\n",
        "\n",
        "**3.4 Mathematical Properties of the Reward Function**\n",
        "\n",
        "The reward function of EOP-PRL has several important mathematical properties:\n",
        "\n",
        "* **Bounded rewards:** All rewards are bounded between $\\beta \\cdot \\gamma$ and $\\alpha$, providing numerical stability during training. The minimum reward occurs for the maximum penalty on an incomplete sequence at the end of training, and the maximum reward occurs for a complete and perfectly matching sequence.\n",
        "* **Monotonicity with sequence length:** For matching tokens, rewards increase monotonically with position, incentivizing the model to maintain correctness throughout longer sequences. This encourages the model to complete the reasoning process correctly.\n",
        "* **Incomplete sequence penalty:** The penalty term creates a sharp discontinuity in the reward between complete and incomplete sequences. This strong negative signal discourages the model from stopping prematurely. The magnitude of the penalty increases with the degree of incompleteness.\n",
        "* **Temporal curriculum:** The penalty scaling function $S(e, E)$ implements a temporal curriculum, with:\n",
        "    * $S(0, E) = 1.0$ (initial scaling, providing mild penalties initially)\n",
        "    * $S(E-1, E) = \\gamma$ (final scaling, providing stronger penalties at the end of training)\n",
        "    * $\\frac{dS(e, E)}{de} = \\frac{\\gamma - 1.0}{E-1} > 0$, ensuring a linear and monotonic increase in penalty scaling throughout training.\n",
        "\n",
        "**4. Theoretical Analysis**\n",
        "\n",
        "**4.1 Exploration-Exploitation Balance**\n",
        "\n",
        "Traditional reinforcement learning for language models faces a fundamental tension between exploration (trying diverse token sequences) and exploitation (following known good paths). We analyze how EOP-PRL addresses this tension.\n",
        "\n",
        "**Theorem 1:** Under the EOP-PRL reward structure, the gradient update for non-matching tokens approaches zero as training progresses, if and only if the generated sequence length approaches the reference length.\n",
        "\n",
        "**Proof:** For a generated token $\\hat{y}_t \\neq y^*_t$, the reward $r_t = 0$ if $|\\hat{y}| \\geq |y^*|$. The gradient contribution to the loss function (Equation 9) for this token is:\n",
        "$$-\\nabla_\\theta (0 \\cdot \\log \\pi_\\theta(\\hat{y}_t | x, \\hat{y}_{<t})) = 0$$\n",
        "Therefore, no explicit gradient signal discourages exploration of alternative tokens, provided the sequence reaches the reference length. The model is free to explore different token choices without being penalized for deviating from the reference during intermediate steps.\n",
        "\n",
        "However, if $|\\hat{y}| < |y^*|$, and $t = |\\hat{y}| - 1$ (the last generated token), the reward includes the penalty term:\n",
        "$$r_t = \\beta \\cdot S(e, E) \\cdot \\frac{|y^*| - |\\hat{y}|}{|y^*|}$$\n",
        "This creates a strong negative reward and thus a significant gradient signal to discourage stopping generation prematurely. The model is penalized for not completing the reasoning path.\n",
        "\n",
        "As $S(e, E)$ increases with the episode number $e$, the pressure to generate complete sequences monotonically increases with training progress. This progressive increase in penalty encourages the model to first learn to complete the sequences and then refine the intermediate steps to match the reference where possible to gain the positive position-scaled rewards.\n",
        "\n",
        "**4.2 Convergence Analysis**\n",
        "\n",
        "**Theorem 2:** Under standard assumptions for policy gradient methods (including finite state and action spaces, bounded rewards, and appropriate learning rate schedules), EOP-PRL converges to a policy that maximizes the expected reward, which favors complete sequences while allowing diversity in token choice.\n",
        "\n",
        "**Proof Outline:**\n",
        "\n",
        "The REINFORCE algorithm provides an unbiased estimate of the gradient of the expected return [5]. With appropriate learning rate schedules that satisfy the Robbins-Monro conditions [13] (i.e., $\\sum_{k=1}^{\\infty} \\eta_k = \\infty$ and $\\sum_{k=1}^{\\infty} \\eta_k^2 < \\infty$, where $\\eta_k$ is the learning rate at iteration $k$), stochastic gradient ascent is guaranteed to converge to a local optimum of the expected return.\n",
        "\n",
        "The reward structure of EOP-PRL is bounded (as established in Section 3.4). The positive rewards are scaled between $0.1\\alpha$ and $\\alpha$, and the negative reward (penalty) is scaled between $\\beta$ and $\\beta \\cdot \\gamma$. This boundedness ensures that the gradient estimates are also bounded, which is a requirement for convergence in many policy gradient theorems.\n",
        "\n",
        "The EOP-PRL reward function is designed to prioritize the generation of complete sequences. The end-only penalty for incomplete sequences provides a strong negative signal, pushing the policy towards generating sequences of length $|y^*|$. Once the model learns to generate complete sequences, the position-scaled reward for matching tokens incentivizes it to align the generated tokens with the reference sequence. However, importantly, there is no penalty for non-matching tokens in complete sequences, allowing for flexibility and exploration of alternative reasoning paths that might also lead to a valid conclusion.\n",
        "\n",
        "Therefore, the expected reward is maximized by policies that generate complete sequences that match the reference as closely as possible. While the proof relies on standard assumptions of policy gradient methods, the specific structure of the EOP-PRL reward function guides the convergence towards policies that achieve the desired behavior of complete and potentially diverse reasoning paths. A more rigorous proof would involve formally showing that the expected reward landscape under EOP-PRL has desirable properties (e.g., unimodality around optimal policies within the space of complete sequences), but this is beyond the scope of this initial analysis.\n",
        "\n",
        "**4.3 Curriculum Learning Effects**\n",
        "\n",
        "**Theorem 3:** The progressive penalty scaling in EOP-PRL implements an effective curriculum learning mechanism that gradually increases performance standards.\n",
        "\n",
        "**Proof:** The penalty scaling function $S(e, E)$ has the following properties:\n",
        "\n",
        "* $S(0, E) = 1.0$, providing mild penalties initially in training episodes.\n",
        "* $S(E-1, E) = \\gamma$, providing stronger penalties at the end of training.\n",
        "* $\\frac{dS(e, E)}{de} = \\frac{\\gamma - 1.0}{E-1} > 0$, ensuring a linear and monotonic increase in the penalty scaling factor as training progresses.\n",
        "\n",
        "These properties ensure that the model initially learns under more lenient conditions, where the penalty for incomplete sequences is relatively small. This encourages the model to explore and learn the basic structure of the reasoning task without being overly penalized for premature termination. As training progresses, the increasing penalty forces the model to focus on generating complete sequences. This gradual increase in the strictness of the completeness requirement aligns perfectly with the definition of curriculum learning [9], where the model learns simpler aspects of the task first before tackling the more challenging requirement of generating complete and correct reasoning paths.\n",
        "\n",
        "**4.4 Comparison with Token-Level Penalties**\n",
        "\n",
        "To highlight the differences between EOP-PRL and traditional token-level penalty approaches, we define a standard token-level penalty function:\n",
        "$$r^{std}_t = \\begin{cases}\n",
        "\\alpha, & \\text{if } \\hat{y}_t = y^*_t \\\\\n",
        "\\beta, & \\text{otherwise}\n",
        "\\end{cases} \\quad (11)$$\n",
        "\n",
        "**Theorem 4:** The magnitude of the expected gradient update for generating an incorrect token is significantly lower in EOP-PRL compared to standard token-level penalty approaches during the intermediate generation steps, leading to greater exploration.\n",
        "\n",
        "**Proof:** For a token where $\\hat{y}_t \\neq y^*_t$:\n",
        "\n",
        "* Under the standard approach: $r^{std}_t = \\beta$, leading to a gradient contribution proportional to $\\beta \\cdot \\nabla_\\theta \\log \\pi_\\theta(\\hat{y}_t | x, \\hat{y}_{<t})$. This directly discourages the generation of any token that does not match the reference at each step.\n",
        "\n",
        "* Under EOP-PRL: $r_t = 0$ for all non-matching tokens in a generated sequence, as long as the sequence eventually reaches the reference length. The penalty is only applied at the very end if the sequence is incomplete. Therefore, for the vast majority of exploratory tokens generated in complete (or potentially complete) sequences, EOP-PRL applies no penalty, resulting in a gradient contribution of zero for these steps. This allows the model to explore different token choices without immediate negative feedback.\n",
        "\n",
        "This lower gradient magnitude for exploratory tokens in EOP-PRL allows for greater diversity in the generated intermediate reasoning steps compared to the standard token-level penalty approach, which strongly discourages any deviation from the reference sequence at every step.\n",
        "\n",
        "**5. Implementation Considerations**\n",
        "\n",
        "**5.1 Gradient Accumulation**\n",
        "\n",
        "For practical implementation, EOP-PRL can effectively utilize gradient accumulation over $G$ steps before updating the policy network parameters. This technique is mathematically equivalent to using a batch size of $G$ but requires less memory, making it suitable for training large language models. For a sequence of losses $L_1, L_2, ..., L_G$ calculated from $G$ independent trajectories, the accumulated gradient is:\n",
        "$$\\nabla_\\theta L_{accumulated} = \\frac{1}{G} \\sum_{i=1}^{G} \\nabla_\\theta L_i \\quad (12)$$\n",
        "This averaged gradient is then used to update the model's parameters.\n",
        "\n",
        "**5.2 Memory-Efficient Attention**\n",
        "\n",
        "When implementing EOP-PRL for large language models, memory efficiency is crucial due to the computational cost of attention mechanisms, especially for long sequences. We can employ a chunked attention computation approach that maintains mathematical equivalence to standard attention while reducing peak memory requirements. For a query matrix $Q \\in \\mathbb{R}^{B \\times T \\times H \\times D}$ and key matrix $K \\in \\mathbb{R}^{B \\times T \\times H \\times D}$, where $B$ is batch size, $T$ is sequence length, $H$ is number of heads, and $D$ is head dimension, the attention scores are typically computed as:\n",
        "$$A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) \\quad (13)$$\n",
        "By chunking the sequence length $T$ into blocks of size $C$, the attention computation can be performed block by block. For example, for a given query chunk $Q_{i:i+C}$, the attention scores are computed with all key chunks $K_{j:j+C}$:\n",
        "$$A_{i,j} = \\text{softmax}\\left(\\frac{Q_{i:i+C} K_{j:j+C}^T}{\\sqrt{d}}\\right) \\quad (14)$$\n",
        "This reduces the peak memory requirement from $\\mathcal{O}(T^2)$ for the full attention matrix to roughly $\\mathcal{O}(C \\cdot T)$, where $C$ is the chunk size, allowing for training with longer sequences on limited computational resources. The choice of chunk size $C$ involves a trade-off between memory reduction and potential computational overhead due to the iterative nature of the chunked computation.\n",
        "\n",
        "**5.3 Hyperparameter Tuning and Stopping Criterion**\n",
        "\n",
        "The performance of EOP-PRL, like other reinforcement learning algorithms, can be sensitive to the choice of hyperparameters. Key hyperparameters such as the base reward value ($\\alpha$), the base penalty value ($\\beta$), the maximum penalty scaling factor ($\\gamma$), and the learning rate will likely require careful tuning based on the specific reasoning task and the language model being used. Additionally, a suitable stopping criterion for the training process needs to be defined, which could be based on the performance on a validation set or reaching a certain number of training episodes.\n",
        "\n",
        "**6. Discussion**\n",
        "\n",
        "**6.1 Advantages over Traditional RL Approaches**\n",
        "\n",
        "The mathematical formulation of EOP-PRL offers several advantages over traditional RL approaches for language model fine-tuning:\n",
        "\n",
        "* **Exploration-friendly reward structure:** By removing penalties for token-level mismatches during generation, EOP-PRL allows models to explore diverse reasoning paths without immediate punishment, as long as the sequences eventually reach the desired length. This is particularly beneficial for tasks where multiple valid reasoning paths exist.\n",
        "* **Completion-focused learning:** The end-only penalty creates a mathematically precise incentive for models to complete their reasoning rather than stopping prematurely. The magnitude of the penalty scales with the degree of incompleteness, encouraging the model to generate longer and more complete sequences.\n",
        "* **Automatic curriculum:** The progressive penalty scaling creates a mathematical curriculum where the standard for sequence completeness gradually increases as training progresses, without requiring manual curriculum design or pre-defined difficulty levels. The model starts by learning to generate any reasonable sequence and is progressively pushed to generate complete ones.\n",
        "* **Memory efficiency:** The implementation considerations, such as gradient accumulation and chunked attention, allow for training with limited computational resources while maintaining mathematical rigor and enabling the application of EOP-PRL to large language models and long reasoning sequences.\n",
        "\n",
        "**6.2 Limitations and Future Work**\n",
        "\n",
        "While mathematically elegant, EOP-PRL has several limitations that warrant further investigation:\n",
        "\n",
        "* **Reference length dependency:** The end-only penalty depends on the reference sequence length $|y^*|$, which might not always be the optimal target length for all prompts. For some prompts, a shorter or longer correct reasoning path might exist. Future work could explore methods for dynamically determining or adapting the target sequence length.\n",
        "* **Linear scaling assumption:** The penalty scaling function $S(e, E)$ assumes a linear increase in standards is optimal. Non-linear scaling functions or adaptive scaling strategies based on the model's performance could potentially lead to faster or more stable training.\n",
        "* **Gradient sparsity:** The zero rewards for non-matching tokens during generation can lead to sparse gradients, especially in the early stages of training before the model starts generating sequences close to the reference length. This sparsity could potentially slow down the learning process in some cases. Future research could explore techniques to mitigate gradient sparsity, such as incorporating small shaping rewards for certain intermediate steps without overly restricting exploration.\n",
        "* **Task Specificity:** The current formulation is primarily designed for tasks where the goal is to generate a complete reasoning path with a defined length. Its applicability to tasks with more open-ended or variable-length outputs needs further investigation.\n",
        "\n",
        "Future work should focus on:\n",
        "\n",
        "* Empirical evaluation of EOP-PRL on a variety of structured reasoning tasks and comparison with existing state-of-the-art RL fine-tuning methods.\n",
        "* Investigating the impact of different forms of the penalty scaling function (e.g., non-linear, adaptive).\n",
        "* Exploring techniques to address the potential issue of gradient sparsity.\n",
        "* Extending the EOP-PRL framework to handle reasoning tasks with variable or unknown optimal output lengths.\n",
        "* Analyzing the diversity of reasoning paths learned by models trained with EOP-PRL.\n",
        "\n",
        "**7. Conclusion**\n",
        "\n",
        "This paper has presented a comprehensive mathematical analysis of End-Only Penalty Progressive Reinforcement Learning (EOP-PRL), demonstrating its theoretical foundations and advantages for training language models on reasoning tasks. By reformulating the reward structure to penalize only incomplete sequences rather than token-level mismatches, EOP-PRL creates a mathematically sound framework that encourages exploration of diverse reasoning paths while ensuring complete responses.\n",
        "\n",
        "The mathematical analysis reveals that EOP-PRL effectively addresses the exploration-exploitation trade-off inherent in language model fine-tuning, implements a natural curriculum through progressive penalty scaling, and converges (under standard assumptions) to policies that prioritize the generation of complete reasoning paths while allowing for greater diversity in token selection. These properties make EOP-PRL particularly well-suited for complex reasoning tasks where multiple valid reasoning paths may lead to correct conclusions.\n",
        "\n",
        "Future work will focus on extending the mathematical framework to address the identified limitations and exploring applications to a broader range of language model training scenarios.\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Brown, T. B., et al. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems*, *33*, 1877-1901.\n",
        "[2] Stiennon, N., et al. (2020). Learning to summarize from human feedback. *Advances in Neural Information Processing Systems*, *33*, 3008-3021.\n",
        "[3] Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. *Advances in Neural Information Processing Systems*, *35*.\n",
        "[4] Sutton, R. S., et al. (2000). Policy gradient methods for reinforcement learning with function approximation. *Advances in Neural Information Processing Systems*, *12*.\n",
        "[5] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. *Machine Learning*, *8*(3-4), 229-256.\n",
        "[6] Christiano, P. F., et al. (2017). Deep reinforcement learning from human preferences. *Advances in Neural Information Processing Systems*, *30*.\n",
        "[7] Schulman, J., et al. (2017). Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*.\n",
        "[8] Jaques, N., et al. (2017). Sequence tutor: Conservative fine-tuning of sequence generation models with KL-control. *International Conference on Machine Learning*.\n",
        "[9] Bengio, Y., et al. (2009). Curriculum learning. *International Conference on Machine Learning*.\n",
        "[10] Graves, A., et al. (2017). Automated curriculum learning for neural networks. *International Conference on Machine Learning*.\n",
        "[11] Ng, A. Y., et al. (1999). Policy invariance under reward transformations: Theory and application to reward shaping. *International Conference on Machine Learning*.\n",
        "[12] Sukhbaatar, S., et al. (2018). Intrinsic motivation and automatic curricula via asymmetric self-play. *International Conference on Learning Representations*.\n",
        "[13] Robbins, H., & Monro, S. (1951). A stochastic approximation method. *The Annals of Mathematical Statistics*, *22*(3), 400-407."
      ],
      "metadata": {
        "id": "15JkgtMRCIfn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0kAFX9jCGm0"
      },
      "outputs": [],
      "source": [
        "## eop pRL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from torch.amp import autocast, GradScaler  # For mixed precision\n",
        "import gc  # For garbage collection\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 1) Hyperparameters\n",
        "# ==========================================\n",
        "hyperparams = {\n",
        "    # Model Architecture\n",
        "    'block_size': 1024,               # Sequence length for context\n",
        "    'batch_size': 1,                  # Batch size (reduced from 2 to save memory)\n",
        "    'embed_dim': 1024,                # Transformer embedding dimension\n",
        "    'n_heads': 16,                    # Number of attention heads\n",
        "    'n_layers': 24,                   # Number of Transformer blocks\n",
        "    'memory_n_layers': 8,             # Number of layers in the original MemoryModule\n",
        "    'vocab_size': 256,                # Fixed vocabulary size for byte tokenization\n",
        "\n",
        "    # Memory Efficiency Settings\n",
        "    'use_gradient_checkpointing': True,  # Use gradient checkpointing\n",
        "    'gradient_accumulation_steps': 4,    # Accumulate gradients for X steps\n",
        "    'chunk_size': 64,                    # Size of chunks for attention calculation\n",
        "    'use_dynamic_quantization': True,    # Use dynamic quantization\n",
        "    'limit_attention_memory': True,      # Use memory-efficient attention implementation\n",
        "\n",
        "    # RL Training Parameters\n",
        "    'n_prompt_ans_pairs': 5,          # Number of prompt-answer pairs to use for RL training\n",
        "    'number_of_practice': 100,        # Number of practice episodes for RL training\n",
        "    'rl_log_interval': 5,             # Log metrics every X episodes during RL training\n",
        "    'rl_save_interval': 20,           # Save checkpoint every X episodes during RL training\n",
        "    'base_reward': 1.0,               # Base reward value for correct predictions\n",
        "    'base_penalty': -0.5,             # Base penalty value for incorrect predictions\n",
        "    'rl_learning_rate': 1e-6,         # Learning rate for RL fine-tuning\n",
        "    'max_penalty_scale': 2.5,         # Maximum penalty scaling factor for episode progression\n",
        "\n",
        "    # Mixed Precision Parameters\n",
        "    'use_mixed_precision': True,      # Whether to use mixed precision training\n",
        "    'grad_scale_init': 65536.0,       # Initial scale for gradient scaler\n",
        "    'scale_growth_interval': 2000,    # Steps between gradient scaler growth\n",
        "\n",
        "    # Generation Parameters\n",
        "    'generate_num_tokens': 2048,      # Number of tokens to generate after each epoch\n",
        "    'top_p': 0.8,                     # Top-p (nucleus) sampling parameter\n",
        "    'start_prompt': \"Explain why the statement 'I wore my lucky socks today, and I got an A on my test, so my socks must be lucky' is a logical fallacy.\",\n",
        "\n",
        "    # Special Tokens & Tags\n",
        "    'thinking_tag': \"<think>\",        # Opening tag for thinking process\n",
        "    'thinking_end_tag': \"</think>\",   # Closing tag for thinking process\n",
        "    'answer_tag': \"<answer>\",         # Opening tag for final answer\n",
        "    'answer_end_tag': \"</answer>\",    # Closing tag for final answer\n",
        "    'bos_token': 254,                 # Beginning-of-sequence token (byte value)\n",
        "    'eos_token': 255,                 # End-of-sequence token (byte value)\n",
        "\n",
        "    # File Paths\n",
        "    'pretrained_model_path': \"threshold_transformer_checkpoint.pt\",  # Path to load pretrained model\n",
        "    'rl_checkpoint_path': \"rl_transformer_checkpoint.pt\",        # RL checkpoint path\n",
        "\n",
        "    # System Prompt\n",
        "    'system_prompt': \"\"\"just think before answer.\"\"\"\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 1.1) Select device and optimize settings\n",
        "# ==========================================\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \\\n",
        "         (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enable tensor cores for better performance with mixed precision\n",
        "if device == \"cuda\" and torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    print(\"TF32 enabled for better performance\")\n",
        "\n",
        "    # Set up GPU for maximum memory efficiency\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"CUDA memory allocated before starting: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "    print(f\"CUDA memory reserved before starting: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
        "\n",
        "# ==========================================\n",
        "# 1.2) Memory Management Functions\n",
        "# ==========================================\n",
        "def clear_memory():\n",
        "    \"\"\"Force clear CUDA memory and run garbage collection.\"\"\"\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "def print_memory_stats():\n",
        "    \"\"\"Print current memory usage statistics.\"\"\"\n",
        "    if device == \"cuda\":\n",
        "        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "        print(f\"CUDA memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
        "        print(f\"CUDA max memory allocated: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "# ==========================================\n",
        "# 1.3) Data Loading and Preprocessing for COT Logic Reasoning\n",
        "# ==========================================\n",
        "def load_cot_logic_data():\n",
        "    print(\"Loading COT Logic Reasoning dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Try standard pandas read_parquet first\n",
        "        df = pd.read_parquet(\"isaiahbjork/cot-logic-reasoning/cot-logic-reasoning.parquet\")\n",
        "        print(\"Dataset loaded using standard path\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset with standard path: {e}\")\n",
        "        try:\n",
        "            # Try with datasets library if available\n",
        "            try:\n",
        "                from datasets import load_dataset\n",
        "                dataset = load_dataset(\"isaiahbjork/cot-logic-reasoning\")\n",
        "                df = dataset[\"train\"].to_pandas()\n",
        "                print(\"Dataset loaded using datasets library\")\n",
        "            except:\n",
        "                # If all else fails, use the original path format\n",
        "                df = pd.read_parquet(\"hf://datasets/isaiahbjork/cot-logic-reasoning/cot-logic-reasoning.parquet\")\n",
        "                print(\"Dataset loaded using hf:// protocol\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Failed to load dataset: {e2}\")\n",
        "            raise RuntimeError(\"Unable to load the COT Logic Reasoning dataset\")\n",
        "\n",
        "    print(f\"Data size: {len(df)}\")\n",
        "\n",
        "    # Split into train/validation/test sets (80/10/10)\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    print(f\"Training examples: {len(train_df)}\")\n",
        "    print(f\"Validation examples: {len(val_df)}\")\n",
        "    print(f\"Test examples: {len(test_df)}\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# ==========================================\n",
        "# 2) Improved Emergent Threshold Layer with Numerical Stability\n",
        "# ==========================================\n",
        "class ImprovedEmergentThresholdLayer(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.norm = nn.LayerNorm(feature_dim)\n",
        "        self.register_buffer('running_mean', torch.zeros(feature_dim))\n",
        "        self.register_buffer('running_var', torch.ones(feature_dim))\n",
        "        self.adaptive_threshold = nn.Parameter(torch.ones(1) * 0.5)\n",
        "        self.momentum = 0.01\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = self.norm(x)\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                batch_mean = x_norm.mean(dim=(0, 1))\n",
        "                batch_var = x_norm.var(dim=(0, 1), unbiased=False)\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "        # More robust threshold calculation with clamping to prevent extremely small values\n",
        "        threshold = torch.sigmoid(self.adaptive_threshold) * torch.sqrt(torch.clamp(self.running_var, min=1e-6))\n",
        "\n",
        "        # Increase denominator from 0.1 to 1.0 for stability\n",
        "        gate = torch.sigmoid((torch.abs(x_norm) - threshold.view(1, 1, -1)) / 1.0)\n",
        "\n",
        "        alpha = torch.sigmoid(self.adaptive_threshold)\n",
        "\n",
        "        # Clip outputs to prevent extreme values\n",
        "        return torch.clamp(alpha * (gate * x) + (1 - alpha) * x, min=-100, max=100)\n",
        "\n",
        "# ==========================================\n",
        "# 3) Memory-Efficient Attention Mechanism\n",
        "# ==========================================\n",
        "class MemoryEfficientAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        # Standard attention projections\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Attention score normalization\n",
        "        self.attn_scale = nn.Parameter(torch.ones(1) * (1.0 / math.sqrt(self.head_dim)))\n",
        "\n",
        "        # Threshold parameters for attention scores\n",
        "        self.register_buffer('score_running_mean', torch.zeros(n_heads))\n",
        "        self.register_buffer('score_running_var', torch.ones(n_heads))\n",
        "        self.score_threshold = nn.Parameter(torch.ones(1) * 0.5)\n",
        "        self.score_momentum = 0.01\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Project to queries, keys, values\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "\n",
        "        # Super memory-efficient attention implementation\n",
        "        # Process in small chunks for both query and key sequences\n",
        "        chunk_size = hyperparams['chunk_size']\n",
        "        attn_output = torch.zeros_like(q)\n",
        "\n",
        "        for i in range(0, T, chunk_size):\n",
        "            i_end = min(i + chunk_size, T)\n",
        "\n",
        "            # Get current query chunk\n",
        "            q_chunk = q[:, :, i:i_end]\n",
        "\n",
        "            # Compute scores for this chunk against all keys, in smaller sub-chunks\n",
        "            scores_for_chunk = []\n",
        "            for j in range(0, T, chunk_size):\n",
        "                j_end = min(j + chunk_size, T)\n",
        "\n",
        "                # Get key chunk and compute scores\n",
        "                k_chunk = k[:, :, j:j_end]\n",
        "                chunk_scores = torch.matmul(q_chunk, k_chunk.transpose(-2, -1)) * self.attn_scale\n",
        "\n",
        "                # Apply causal mask if needed - only allow attention to previous positions\n",
        "                if attn_mask is not None and i >= j:\n",
        "                    # Generate mask just for this chunk\n",
        "                    mask_size = (i_end-i, j_end-j)\n",
        "                    chunk_mask = torch.triu(torch.ones(mask_size, device=x.device), diagonal=j-i+1).bool()\n",
        "                    chunk_mask = chunk_mask.unsqueeze(0).unsqueeze(0).expand(B, self.n_heads, -1, -1)\n",
        "                    chunk_scores.masked_fill_(chunk_mask, float('-inf'))\n",
        "\n",
        "                scores_for_chunk.append(chunk_scores)\n",
        "\n",
        "            # Concatenate all key chunks for this query chunk\n",
        "            all_scores_for_chunk = torch.cat(scores_for_chunk, dim=-1)\n",
        "\n",
        "            # Apply softmax across the full key dimension\n",
        "            attn_weights = F.softmax(all_scores_for_chunk, dim=-1)\n",
        "\n",
        "            # Multiply with values in chunks\n",
        "            chunk_output = torch.zeros_like(q_chunk)\n",
        "            start_idx = 0\n",
        "            for j in range(0, T, chunk_size):\n",
        "                j_end = min(j + chunk_size, T)\n",
        "\n",
        "                # Get weights for this chunk and the corresponding values\n",
        "                weights_chunk = attn_weights[:, :, :, start_idx:start_idx + (j_end - j)]\n",
        "                v_chunk = v[:, :, j:j_end]\n",
        "\n",
        "                # Accumulate the output for this chunk\n",
        "                chunk_output += torch.matmul(weights_chunk, v_chunk)\n",
        "                start_idx += (j_end - j)\n",
        "\n",
        "            # Place the output for this query chunk in the right position\n",
        "            attn_output[:, :, i:i_end] = chunk_output\n",
        "\n",
        "        # Reshape output back to original dimensions\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "    # Method to handle compatibility with original MultiheadAttention\n",
        "    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n",
        "        # Map old MHA parameters to new ThresholdedAttention parameters\n",
        "        if f\"{prefix}in_proj_weight\" in state_dict:\n",
        "            # MultiheadAttention uses a single in_proj_weight that combines q,k,v\n",
        "            in_proj_weight = state_dict.pop(f\"{prefix}in_proj_weight\")\n",
        "            in_proj_bias = state_dict.pop(f\"{prefix}in_proj_bias\", None)\n",
        "\n",
        "            # Split the in_proj_weight into q, k, v parts\n",
        "            q_weight, k_weight, v_weight = in_proj_weight.chunk(3, dim=0)\n",
        "            state_dict[f\"{prefix}q_proj.weight\"] = q_weight\n",
        "            state_dict[f\"{prefix}k_proj.weight\"] = k_weight\n",
        "            state_dict[f\"{prefix}v_proj.weight\"] = v_weight\n",
        "\n",
        "            if in_proj_bias is not None:\n",
        "                q_bias, k_bias, v_bias = in_proj_bias.chunk(3, dim=0)\n",
        "                state_dict[f\"{prefix}q_proj.bias\"] = q_bias\n",
        "                state_dict[f\"{prefix}k_proj.bias\"] = k_bias\n",
        "                state_dict[f\"{prefix}v_proj.bias\"] = v_bias\n",
        "\n",
        "        # Map out_proj parameters\n",
        "        if f\"{prefix}out_proj.weight\" in state_dict:\n",
        "            state_dict[f\"{prefix}out_proj.weight\"] = state_dict[f\"{prefix}out_proj.weight\"]\n",
        "            if f\"{prefix}out_proj.bias\" in state_dict:\n",
        "                state_dict[f\"{prefix}out_proj.bias\"] = state_dict[f\"{prefix}out_proj.bias\"]\n",
        "\n",
        "        # Call parent class method to handle the rest\n",
        "        super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n",
        "\n",
        "# ==========================================\n",
        "# 4) Improved Transformer Block with Memory Efficiency\n",
        "# ==========================================\n",
        "class ImprovedTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.attention = MemoryEfficientAttention(embed_dim, n_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            ImprovedEmergentThresholdLayer(4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        self.threshold1 = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "        self.threshold2 = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use sequential processing to reduce memory usage\n",
        "        attn_out = self.attention(x)\n",
        "        x = x + self.threshold1(attn_out)\n",
        "\n",
        "        # Explicitly delete to free memory\n",
        "        del attn_out\n",
        "\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = x + self.threshold2(ff_out)\n",
        "\n",
        "        # Explicitly delete to free memory\n",
        "        del ff_out\n",
        "\n",
        "        return x\n",
        "\n",
        "# ==========================================\n",
        "# 5) Improved Byte Transformer with Gradient Checkpointing\n",
        "# ==========================================\n",
        "class ImprovedByteTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, n_heads=4, n_layers=4, block_size=128):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(self.block_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ImprovedTransformerBlock(embed_dim, n_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.final_threshold = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "        self.ln_f = nn.Linear(embed_dim, vocab_size)\n",
        "        # Learned gating parameter for combining memory outputs\n",
        "        self.gate_param = nn.Parameter(torch.tensor(0.0))\n",
        "        self.use_checkpointing = hyperparams['use_gradient_checkpointing']\n",
        "\n",
        "    def forward_with_embeddings(self, x_emb):\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            if self.use_checkpointing and self.training:\n",
        "                # Ensure tensor requires gradients for checkpointing\n",
        "                if not x_emb.requires_grad:\n",
        "                    x_emb.requires_grad = True\n",
        "                x_emb = torch.utils.checkpoint.checkpoint(block, x_emb, use_reentrant=False)\n",
        "            else:\n",
        "                x_emb = block(x_emb)\n",
        "        x_emb = self.final_threshold(x_emb)\n",
        "        logits = self.ln_f(x_emb)\n",
        "        return logits\n",
        "\n",
        "    def forward_with_two_memory(self, x_emb, memory_module2):\n",
        "        \"\"\"\n",
        "        Extended forward pass with memory modules and gradient checkpointing\n",
        "        \"\"\"\n",
        "        transformer_out = x_emb\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            if self.use_checkpointing and self.training:\n",
        "                # Ensure tensor requires gradients for checkpointing\n",
        "                if not transformer_out.requires_grad:\n",
        "                    transformer_out.requires_grad = True\n",
        "                transformer_out = torch.utils.checkpoint.checkpoint(block, transformer_out, use_reentrant=False)\n",
        "            else:\n",
        "                transformer_out = block(transformer_out)\n",
        "\n",
        "        transformer_out = self.final_threshold(transformer_out)\n",
        "\n",
        "        if self.use_checkpointing and self.training:\n",
        "            # Ensure tensor requires gradients for checkpointing\n",
        "            if not transformer_out.requires_grad:\n",
        "                transformer_out.requires_grad = True\n",
        "            mem_out2 = torch.utils.checkpoint.checkpoint(memory_module2, transformer_out, use_reentrant=False)\n",
        "        else:\n",
        "            mem_out2 = memory_module2(transformer_out)\n",
        "\n",
        "        # Gated combination\n",
        "        alpha = torch.sigmoid(self.gate_param)\n",
        "        combined = alpha * mem_out2 + (1 - alpha) * x_emb\n",
        "        final_emb = self.final_threshold(combined)\n",
        "        logits = self.ln_f(final_emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.size()\n",
        "        token_emb = self.token_embedding(x)\n",
        "        positions = torch.arange(min(T, self.block_size), device=x.device).unsqueeze(0)\n",
        "        pos_emb = self.pos_embedding(positions)\n",
        "        x_emb = token_emb[:, :min(T, self.block_size)] + pos_emb\n",
        "        return self.forward_with_embeddings(x_emb)\n",
        "\n",
        "# ==========================================\n",
        "# 6) Memory Module with Gradient Checkpointing\n",
        "# ==========================================\n",
        "class MemoryModule(nn.Module):\n",
        "    def __init__(self, embed_dim, n_layers=8, expansion_factor=4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            layer = nn.Sequential(\n",
        "                nn.LayerNorm(embed_dim),\n",
        "                nn.Linear(embed_dim, embed_dim * expansion_factor),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(embed_dim * expansion_factor, embed_dim),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "        self.final_norm = nn.LayerNorm(embed_dim)\n",
        "        self.use_checkpointing = hyperparams['use_gradient_checkpointing']\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            if self.use_checkpointing and self.training:\n",
        "                # Ensure tensor requires gradients for checkpointing\n",
        "                if not out.requires_grad:\n",
        "                    out.requires_grad = True\n",
        "                residual = torch.utils.checkpoint.checkpoint(layer, out, use_reentrant=False)\n",
        "                out = out + residual\n",
        "            else:\n",
        "                out = out + layer(out)\n",
        "        out = self.final_norm(out)\n",
        "        return out\n",
        "\n",
        "# ==========================================\n",
        "# 8) Progressive Reward RL Training with Gradient Accumulation and End-Only Penalty\n",
        "# ==========================================\n",
        "class ProgressiveRewardTrainer:\n",
        "    def __init__(self, main_model, memory1, memory2,\n",
        "                 base_reward=1.0, base_penalty=-0.5,\n",
        "                 learning_rate=5e-6, max_penalty_scale=2.5):\n",
        "        self.main_model = main_model\n",
        "        self.memory1 = memory1\n",
        "        self.memory2 = memory2\n",
        "        self.base_reward = base_reward\n",
        "        self.base_penalty = base_penalty\n",
        "        self.max_penalty_scale = max_penalty_scale\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            list(main_model.parameters()) +\n",
        "            list(memory1.parameters()) +\n",
        "            list(memory2.parameters()),\n",
        "            lr=learning_rate\n",
        "        )\n",
        "        # Create gradient scaler for mixed precision training\n",
        "        self.scaler = GradScaler(\n",
        "            init_scale=hyperparams['grad_scale_init'],\n",
        "            growth_interval=hyperparams['scale_growth_interval'],\n",
        "            enabled=hyperparams['use_mixed_precision']\n",
        "        )\n",
        "        # Gradient accumulation steps\n",
        "        self.gradient_accumulation_steps = hyperparams['gradient_accumulation_steps']\n",
        "        # Track accumulated batches\n",
        "        self.accumulated_batches = 0\n",
        "        # Store metrics for visualization\n",
        "        self.metrics_history = {\n",
        "            'episodes': [],\n",
        "            'penalty_scale': [],\n",
        "            'policy_loss': [],\n",
        "            'avg_reward': []\n",
        "        }\n",
        "\n",
        "    def calculate_episode_penalty_scaling(self, current_episode, total_episodes):\n",
        "        \"\"\"Calculate the penalty scaling factor based on episode progress.\"\"\"\n",
        "        # Ensure the scaling starts at 1.0 and linearly increases to max_scale\n",
        "        scale = 1.0 + (current_episode / max(1, total_episodes - 1)) * (self.max_penalty_scale - 1.0)\n",
        "        return scale\n",
        "\n",
        "    def compute_progressive_rewards(self, generated_tokens, reference_tokens, penalty_scale=1.0):\n",
        "        \"\"\"\n",
        "        Compute rewards that only penalize for incomplete completions at the end.\n",
        "        Matching tokens still receive rewards, but non-matching tokens don't get penalties.\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        gen_len = len(generated_tokens)\n",
        "        ref_len = len(reference_tokens)\n",
        "        compare_len = min(gen_len, ref_len)\n",
        "\n",
        "        # For tokens that exist in both sequences, only give rewards for matches\n",
        "        for i in range(compare_len):\n",
        "            # Position-based scaling factor (increases from 0.1 to 1.0)\n",
        "            position_scale = 0.1 + 0.9 * (i / max(gen_len, 1))\n",
        "\n",
        "            # Only rewards for matching tokens, no penalties for mismatches\n",
        "            if generated_tokens[i] == reference_tokens[i]:\n",
        "                reward = self.base_reward * position_scale\n",
        "            else:\n",
        "                # No penalty for incorrect tokens during generation\n",
        "                reward = 0.0\n",
        "\n",
        "            rewards.append(reward)\n",
        "\n",
        "        # Only penalize if the generation is incomplete (shorter than reference)\n",
        "        if gen_len < ref_len:\n",
        "            # Calculate how many tokens are missing\n",
        "            missing_tokens = ref_len - gen_len\n",
        "\n",
        "            # Calculate the severity of incompleteness (higher if more is missing)\n",
        "            incompleteness_ratio = missing_tokens / ref_len\n",
        "\n",
        "            # Add a single penalty at the end for the incomplete generation\n",
        "            # Scale by both episode progress and degree of incompleteness\n",
        "            end_penalty = self.base_penalty * penalty_scale * incompleteness_ratio\n",
        "\n",
        "            # Add to the last token's reward (or append if empty)\n",
        "            if rewards:\n",
        "                rewards[-1] += end_penalty\n",
        "            else:\n",
        "                rewards.append(end_penalty)\n",
        "\n",
        "        return torch.tensor(rewards, device=device)\n",
        "\n",
        "    def train_step(self, prompt, reference_answer, current_episode=0, total_episodes=1):\n",
        "        \"\"\"Execute one REINFORCE training step with progressive rewards and gradient accumulation.\"\"\"\n",
        "        # Calculate penalty scaling based on episode progress\n",
        "        penalty_scale = self.calculate_episode_penalty_scaling(current_episode, total_episodes)\n",
        "\n",
        "        # Ensure models are in training mode\n",
        "        self.main_model.train()\n",
        "        self.memory1.train()\n",
        "        self.memory2.train()\n",
        "\n",
        "        # Only zero gradients at the start of accumulation\n",
        "        if self.accumulated_batches == 0:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        # Prepare input context\n",
        "        system_prompt = hyperparams['system_prompt']\n",
        "        full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt}\"\n",
        "        prompt_bytes = full_prompt.encode('utf-8')\n",
        "\n",
        "        # Add BOS token to start generation\n",
        "        prompt_tokens = list(prompt_bytes) + [hyperparams['bos_token']]\n",
        "        context = torch.tensor([prompt_tokens], dtype=torch.long, device=device)\n",
        "\n",
        "        # Reference answer tokens\n",
        "        ref_tokens = list(reference_answer.encode('utf-8'))\n",
        "\n",
        "        # Storage for generation\n",
        "        log_probs = []\n",
        "        generated_tokens = []\n",
        "\n",
        "        # Auto-regressive generation with gradient tracking and mixed precision\n",
        "        # Using a smaller max_tokens to save memory\n",
        "        max_tokens = min(4096, len(ref_tokens) * 2)  # Reduced from 512\n",
        "\n",
        "        # Enable mixed precision for forward passes\n",
        "        with autocast(device_type='cuda' if device == 'cuda' else 'cpu', enabled=hyperparams['use_mixed_precision']):\n",
        "            for _ in range(max_tokens):\n",
        "                # Clear CUDA cache periodically during generation\n",
        "                if _ % 50 == 0 and device == \"cuda\":\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                # Get context within block size limit\n",
        "                x_cond = context[:, -hyperparams['block_size']:] if context.size(1) > hyperparams['block_size'] else context\n",
        "\n",
        "                # Get embeddings\n",
        "                B, T = x_cond.shape\n",
        "                token_emb = self.main_model.token_embedding(x_cond)\n",
        "\n",
        "                # Handle the case where T > block_size\n",
        "                effective_T = min(T, self.main_model.block_size)\n",
        "                pos_indices = torch.arange(effective_T, device=x_cond.device).unsqueeze(0)\n",
        "                pos_emb = self.main_model.pos_embedding(pos_indices)\n",
        "\n",
        "                combined_emb = token_emb[:, :effective_T] + pos_emb\n",
        "\n",
        "                # Forward pass through model\n",
        "                mem_out1 = self.memory1(combined_emb)\n",
        "                logits = self.main_model.forward_with_two_memory(mem_out1, self.memory2)\n",
        "\n",
        "                # Get probabilities for next token\n",
        "                next_token_logits = logits[:, -1, :]\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                log_prob_dist = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "                # Sample token\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                token_value = next_token.item()\n",
        "\n",
        "                # Record log probability for policy gradient\n",
        "                token_log_prob = log_prob_dist.gather(1, next_token).squeeze()\n",
        "                log_probs.append(token_log_prob)\n",
        "\n",
        "                # Add token to generated sequence\n",
        "                generated_tokens.append(token_value)\n",
        "                context = torch.cat([context, next_token], dim=1)\n",
        "\n",
        "                # Stop conditions\n",
        "                if token_value == hyperparams['eos_token']:\n",
        "                    break\n",
        "\n",
        "                # Check for answer end tag\n",
        "                try:\n",
        "                    last_tokens = [t for t in context[0, -30:].tolist() if t != 0]\n",
        "                    recent_text = bytes(last_tokens).decode('utf-8', errors='replace')\n",
        "\n",
        "                    if hyperparams['answer_end_tag'] in recent_text:\n",
        "                        full_text = bytes([t for t in context[0].tolist() if t != 0]).decode('utf-8', errors='replace')\n",
        "                        if (hyperparams['thinking_end_tag'] in full_text and\n",
        "                            hyperparams['answer_end_tag'] in full_text):\n",
        "                            break\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        # Calculate progressive rewards with penalty scaling\n",
        "        rewards = self.compute_progressive_rewards(generated_tokens, ref_tokens, penalty_scale)\n",
        "\n",
        "        # Match rewards to log_probs length\n",
        "        if len(rewards) > len(log_probs):\n",
        "            rewards = rewards[:len(log_probs)]\n",
        "        elif len(log_probs) > len(rewards):\n",
        "            log_probs = log_probs[:len(rewards)]\n",
        "\n",
        "        # REINFORCE policy gradient loss with mixed precision handling\n",
        "        loss_metrics = {\"policy_loss\": 0.0, \"avg_reward\": 0.0}\n",
        "\n",
        "        if len(log_probs) > 0 and len(rewards) > 0:\n",
        "            # Use full precision for loss calculation\n",
        "            policy_loss = -torch.sum(torch.stack(log_probs) * rewards)\n",
        "\n",
        "            # Scale loss for gradient accumulation\n",
        "            policy_loss = policy_loss / self.gradient_accumulation_steps\n",
        "\n",
        "            # Use scaler for mixed precision backpropagation\n",
        "            self.scaler.scale(policy_loss).backward()\n",
        "\n",
        "            # Record metrics\n",
        "            loss_metrics[\"policy_loss\"] = policy_loss.item() * self.gradient_accumulation_steps  # Unscale for reporting\n",
        "            loss_metrics[\"avg_reward\"] = rewards.mean().item()\n",
        "\n",
        "            # Increment accumulated batches\n",
        "            self.accumulated_batches += 1\n",
        "\n",
        "            # Update parameters if we've accumulated enough gradients\n",
        "            if self.accumulated_batches >= self.gradient_accumulation_steps:\n",
        "                # Gradient clipping\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(self.main_model.parameters(), 1.0)\n",
        "                torch.nn.utils.clip_grad_norm_(self.memory1.parameters(), 1.0)\n",
        "                torch.nn.utils.clip_grad_norm_(self.memory2.parameters(), 1.0)\n",
        "\n",
        "                # Update with scaler\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                # Reset accumulation counter\n",
        "                self.accumulated_batches = 0\n",
        "\n",
        "        # Clear memory\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return {\n",
        "            'policy_loss': loss_metrics[\"policy_loss\"],\n",
        "            'avg_reward': loss_metrics[\"avg_reward\"],\n",
        "            'generated_length': len(generated_tokens),\n",
        "            'reference_length': len(ref_tokens),\n",
        "            'scaler_scale': self.scaler.get_scale(),\n",
        "            'optimizer_step_taken': self.accumulated_batches == 0,  # True if we just took an optimizer step\n",
        "            'penalty_scale': penalty_scale  # Track the penalty scaling for monitoring\n",
        "        }\n",
        "\n",
        "    def train(self, train_df, num_prompt_pairs=10, num_episodes=100, log_interval=5, save_interval=20):\n",
        "        \"\"\"Run full RL training procedure.\"\"\"\n",
        "        # Sample a fixed set of prompt-answer pairs for training\n",
        "        if len(train_df) < num_prompt_pairs:\n",
        "            print(f\"Warning: Requested {num_prompt_pairs} pairs but dataset only has {len(train_df)} examples\")\n",
        "            selected_indices = list(range(len(train_df)))\n",
        "        else:\n",
        "            selected_indices = random.sample(range(len(train_df)), num_prompt_pairs)\n",
        "\n",
        "        selected_pairs = train_df.iloc[selected_indices]\n",
        "        print(f\"Selected {len(selected_indices)} prompt-answer pairs for RL training\")\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            # Sample random prompt-answer pair from our selected pairs\n",
        "            idx = random.randint(0, len(selected_pairs) - 1)\n",
        "            prompt = selected_pairs.iloc[idx]['prompt']\n",
        "            reference = selected_pairs.iloc[idx]['response']\n",
        "\n",
        "            # Print memory stats before training step\n",
        "            if (episode + 1) % log_interval == 0 and device == \"cuda\":\n",
        "                print_memory_stats()\n",
        "\n",
        "            # Execute training step with episode information\n",
        "            metrics = self.train_step(prompt, reference,\n",
        "                                     current_episode=episode,\n",
        "                                     total_episodes=num_episodes)\n",
        "\n",
        "            # Store metrics for visualization\n",
        "            self.metrics_history['episodes'].append(episode + 1)\n",
        "            self.metrics_history['penalty_scale'].append(metrics['penalty_scale'])\n",
        "            self.metrics_history['policy_loss'].append(metrics['policy_loss'])\n",
        "            self.metrics_history['avg_reward'].append(metrics['avg_reward'])\n",
        "\n",
        "            # Logging\n",
        "            if (episode + 1) % log_interval == 0:\n",
        "                print(f\"Episode {episode+1}/{num_episodes}, Metrics: {metrics}\")\n",
        "\n",
        "            # Save checkpoint\n",
        "            if (episode + 1) % save_interval == 0:\n",
        "                save_path = hyperparams['rl_checkpoint_path'].replace('.pt', f'_ep{episode+1}.pt')\n",
        "                torch.save({\n",
        "                    'main_model_state': self.main_model.state_dict(),\n",
        "                    'memory1_state': self.memory1.state_dict(),\n",
        "                    'memory2_state': self.memory2.state_dict(),\n",
        "                    'episode': episode + 1,\n",
        "                    'scaler': self.scaler.state_dict(),  # Save scaler state\n",
        "                    'metrics_history': self.metrics_history  # Save metrics for visualization\n",
        "                }, save_path)\n",
        "                print(f\"Checkpoint saved to {save_path}\")\n",
        "\n",
        "                # Visualize penalty effect after saving checkpoint\n",
        "                if episode + 1 >= log_interval:\n",
        "                    self.visualize_penalty_effect()\n",
        "\n",
        "                # Force cleanup after checkpoint\n",
        "                clear_memory()\n",
        "\n",
        "    def visualize_penalty_effect(self):\n",
        "        \"\"\"Generate plots to visualize the effect of the progressive penalty scaling.\"\"\"\n",
        "        # Only create visualization if we have enough data points\n",
        "        if len(self.metrics_history['episodes']) < 2:\n",
        "            return\n",
        "\n",
        "        # Create figure with multiple subplots\n",
        "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
        "\n",
        "        # Plot 1: Penalty Scale over episodes\n",
        "        ax1.plot(self.metrics_history['episodes'], self.metrics_history['penalty_scale'],\n",
        "                marker='o', linestyle='-', color='red')\n",
        "        ax1.set_ylabel('Penalty Scale')\n",
        "        ax1.set_title('Progressive Penalty Scaling over Episodes')\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Plot 2: Policy Loss over episodes\n",
        "        ax2.plot(self.metrics_history['episodes'], self.metrics_history['policy_loss'],\n",
        "                marker='x', linestyle='-', color='blue')\n",
        "        ax2.set_ylabel('Policy Loss')\n",
        "        ax2.set_title('Policy Loss over Episodes')\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # Plot 3: Average Reward over episodes\n",
        "        ax3.plot(self.metrics_history['episodes'], self.metrics_history['avg_reward'],\n",
        "                marker='s', linestyle='-', color='green')\n",
        "        ax3.set_xlabel('Episode')\n",
        "        ax3.set_ylabel('Average Reward')\n",
        "        ax3.set_title('Average Reward over Episodes')\n",
        "        ax3.grid(True)\n",
        "\n",
        "        # Adjust layout and save figure\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"penalty_effect_visualization_ep{max(self.metrics_history['episodes'])}.png\")\n",
        "        plt.close(fig)\n",
        "        print(f\"Visualization saved to penalty_effect_visualization_ep{max(self.metrics_history['episodes'])}.png\")\n",
        "\n",
        "# ==========================================\n",
        "# 7) Generate Text from Trained Model with Dynamic Quantization\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def generate_from_prompt(main_model, memory1, memory2, prompt_text=None, max_new_tokens=200, top_p=None):\n",
        "    if prompt_text is None:\n",
        "        prompt_text = hyperparams['start_prompt']\n",
        "\n",
        "    # Use hyperparameter value if top_p not specified\n",
        "    if top_p is None:\n",
        "        top_p = hyperparams['top_p']\n",
        "\n",
        "    # Apply system prompt to user prompt\n",
        "    system_prompt = hyperparams['system_prompt']\n",
        "    full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt_text}\"\n",
        "\n",
        "    # Convert prompt to bytes\n",
        "    if isinstance(full_prompt, str):\n",
        "        prompt_bytes = full_prompt.encode('utf-8')\n",
        "    elif not isinstance(full_prompt, bytes):\n",
        "        prompt_bytes = str(full_prompt).encode('utf-8')\n",
        "\n",
        "    # Only quantize for CPU, not for CUDA (to fix the error)\n",
        "    if hyperparams['use_dynamic_quantization'] and device != \"cuda\":\n",
        "        print(\"Quantizing models for inference...\")\n",
        "        # Quantize main model\n",
        "        quantized_main_model = torch.quantization.quantize_dynamic(\n",
        "            main_model,\n",
        "            {nn.Linear},\n",
        "            dtype=torch.qint8\n",
        "        )\n",
        "        # Quantize memory modules\n",
        "        quantized_memory1 = torch.quantization.quantize_dynamic(\n",
        "            memory1,\n",
        "            {nn.Linear},\n",
        "            dtype=torch.qint8\n",
        "        )\n",
        "        quantized_memory2 = torch.quantization.quantize_dynamic(\n",
        "            memory2,\n",
        "            {nn.Linear},\n",
        "            dtype=torch.qint8\n",
        "        )\n",
        "        # Use quantized models\n",
        "        use_main_model = quantized_main_model\n",
        "        use_memory1 = quantized_memory1\n",
        "        use_memory2 = quantized_memory2\n",
        "        print(\"Models quantized for inference\")\n",
        "    else:\n",
        "        # If on CUDA, dynamic quantization is not supported, so use original models\n",
        "        if hyperparams['use_dynamic_quantization'] and device == \"cuda\":\n",
        "            print(\"Dynamic quantization not supported on CUDA, using original models\")\n",
        "        # Use original models\n",
        "        use_main_model = main_model\n",
        "        use_memory1 = memory1\n",
        "        use_memory2 = memory2\n",
        "\n",
        "    use_main_model.eval()\n",
        "    use_memory1.eval()\n",
        "    use_memory2.eval()\n",
        "\n",
        "    # Create context from prompt\n",
        "    context = torch.tensor([b for b in prompt_bytes], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Add BOS token to start the response generation\n",
        "    bos_token = torch.tensor([[hyperparams['bos_token']]], dtype=torch.long, device=device)\n",
        "    context = torch.cat([context, bos_token], dim=1)\n",
        "\n",
        "    generated = []\n",
        "    eos_found = False\n",
        "\n",
        "    # Generate with reduced batch size and in smaller chunks for memory efficiency\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_found:\n",
        "            break\n",
        "\n",
        "        # Only use the last block_size tokens for context to save memory\n",
        "        x_cond = context[:, -hyperparams['block_size']:] if context.size(1) > hyperparams['block_size'] else context\n",
        "        B, T = x_cond.shape\n",
        "        token_emb = use_main_model.token_embedding(x_cond)\n",
        "\n",
        "        # Handle the case where T > block_size\n",
        "        effective_T = min(T, use_main_model.block_size)\n",
        "        pos_indices = torch.arange(effective_T, device=x_cond.device).unsqueeze(0)\n",
        "        pos_emb = use_main_model.pos_embedding(pos_indices)\n",
        "\n",
        "        combined_emb = token_emb[:, :effective_T] + pos_emb\n",
        "\n",
        "        # Forward pass with memory modules\n",
        "        mem_out1 = use_memory1(combined_emb)\n",
        "        logits = use_main_model.forward_with_two_memory(mem_out1, use_memory2)\n",
        "\n",
        "        # Get next token distribution with top-p (nucleus) sampling\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sort probabilities in descending order\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "        # Compute cumulative probabilities\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Find indices where cumulative probability exceeds top_p\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        # Shift to create first index (0) as False to always keep at least one token\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Create mask for indices to remove\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "\n",
        "        # Filter logits\n",
        "        filtered_logits = logits.clone()\n",
        "        filtered_logits[indices_to_remove] = -float('inf')\n",
        "\n",
        "        # Get probabilities from filtered logits\n",
        "        filtered_probs = F.softmax(filtered_logits, dim=-1)\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        next_token = torch.multinomial(filtered_probs, num_samples=1)\n",
        "        next_token_value = next_token.item()\n",
        "\n",
        "        # Check for EOS token\n",
        "        if next_token_value == hyperparams['eos_token']:\n",
        "            eos_found = True\n",
        "\n",
        "        generated.append(next_token_value)\n",
        "        context = torch.cat([context, next_token], dim=1)\n",
        "\n",
        "        # Free some memory periodically\n",
        "        if _ % 50 == 0 and device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Combine context with generated bytes and return as bytes object\n",
        "    result_bytes = bytes(context.view(-1).tolist())\n",
        "\n",
        "    # Clean up special tokens when returning result\n",
        "    try:\n",
        "        # Convert to list for easier manipulation\n",
        "        byte_list = list(result_bytes)\n",
        "\n",
        "        # Find all BOS tokens and remove them\n",
        "        while hyperparams['bos_token'] in byte_list:\n",
        "            byte_list.remove(hyperparams['bos_token'])\n",
        "\n",
        "        # Find all EOS tokens and remove everything after the first one\n",
        "        if hyperparams['eos_token'] in byte_list:\n",
        "            eos_index = byte_list.index(hyperparams['eos_token'])\n",
        "            byte_list = byte_list[:eos_index]\n",
        "\n",
        "        # Convert back to bytes\n",
        "        cleaned_bytes = bytes(byte_list)\n",
        "        return cleaned_bytes\n",
        "    except:\n",
        "        # If any error in cleaning, return the original bytes\n",
        "        return result_bytes\n",
        "\n",
        "# ==========================================\n",
        "# 9) Main RL Training Function\n",
        "# ==========================================\n",
        "def train_with_progressive_rewards():\n",
        "    \"\"\"Main function to run RL training with progressive rewards.\"\"\"\n",
        "    # Create models with original architecture\n",
        "    main_model = ImprovedByteTransformer(\n",
        "        vocab_size=hyperparams['vocab_size'],\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_heads=hyperparams['n_heads'],\n",
        "        n_layers=hyperparams['n_layers'],\n",
        "        block_size=hyperparams['block_size']\n",
        "    ).to(device)\n",
        "\n",
        "    memory1 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    memory2 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    # Set all models to training mode explicitly\n",
        "    main_model.train()\n",
        "    memory1.train()\n",
        "    memory2.train()\n",
        "\n",
        "    # Calculate model size\n",
        "    num_params = sum(p.numel() for p in main_model.parameters() if p.requires_grad)\n",
        "    num_params += sum(p.numel() for p in memory1.parameters() if p.requires_grad)\n",
        "    num_params += sum(p.numel() for p in memory2.parameters() if p.requires_grad)\n",
        "    print(f\"Total trainable parameters: {num_params:,}\")\n",
        "\n",
        "    # Load pretrained model if available\n",
        "    if os.path.exists(hyperparams['pretrained_model_path']):\n",
        "        print(f\"Loading pretrained model from {hyperparams['pretrained_model_path']}...\")\n",
        "        try:\n",
        "            checkpoint = torch.load(hyperparams['pretrained_model_path'], map_location=device)\n",
        "            main_model.load_state_dict(checkpoint['main_model_state'], strict=False)\n",
        "            memory1.load_state_dict(checkpoint['memory1_state'])\n",
        "            if 'memory2_state' in checkpoint:\n",
        "                memory2.load_state_dict(checkpoint['memory2_state'])\n",
        "            print(\"Pretrained model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading pretrained model: {e}\")\n",
        "            print(\"Starting with randomly initialized weights.\")\n",
        "    else:\n",
        "        print(f\"Warning: No pretrained model found at {hyperparams['pretrained_model_path']}.\")\n",
        "        print(\"Starting with randomly initialized weights.\")\n",
        "\n",
        "    # Enabled gradient checkpointing if requested (saves memory during training)\n",
        "    if hyperparams['use_gradient_checkpointing']:\n",
        "        print(\"Gradient checkpointing enabled for memory efficiency\")\n",
        "\n",
        "    # Load dataset\n",
        "    train_df, val_df, _ = load_cot_logic_data()\n",
        "\n",
        "    # Create RL trainer with the max penalty scale\n",
        "    trainer = ProgressiveRewardTrainer(\n",
        "        main_model=main_model,\n",
        "        memory1=memory1,\n",
        "        memory2=memory2,\n",
        "        base_reward=hyperparams['base_reward'],\n",
        "        base_penalty=hyperparams['base_penalty'],\n",
        "        learning_rate=hyperparams['rl_learning_rate'],\n",
        "        max_penalty_scale=hyperparams['max_penalty_scale']\n",
        "    )\n",
        "\n",
        "    # Run training\n",
        "    trainer.train(\n",
        "        train_df=train_df,\n",
        "        num_prompt_pairs=hyperparams['n_prompt_ans_pairs'],\n",
        "        num_episodes=hyperparams['number_of_practice'],\n",
        "        log_interval=hyperparams['rl_log_interval'],\n",
        "        save_interval=hyperparams['rl_save_interval']\n",
        "    )\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'main_model_state': main_model.state_dict(),\n",
        "        'memory1_state': memory1.state_dict(),\n",
        "        'memory2_state': memory2.state_dict(),\n",
        "        'metrics_history': trainer.metrics_history\n",
        "    }, \"rl_final_model.pt\")\n",
        "\n",
        "    # Generate final visualization\n",
        "    trainer.visualize_penalty_effect()\n",
        "\n",
        "    print(\"RL training complete!\")\n",
        "\n",
        "# ==========================================\n",
        "# 10) Test RL-trained Model\n",
        "# ==========================================\n",
        "def test_rl_model(model_path=\"rl_final_model.pt\"):\n",
        "    \"\"\"Test the RL-trained model on a few examples.\"\"\"\n",
        "    # Create models\n",
        "    main_model = ImprovedByteTransformer(\n",
        "        vocab_size=hyperparams['vocab_size'],\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_heads=hyperparams['n_heads'],\n",
        "        n_layers=hyperparams['n_layers'],\n",
        "        block_size=hyperparams['block_size']\n",
        "    ).to(device)\n",
        "\n",
        "    memory1 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    memory2 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    # Load trained model\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading RL-trained model from {model_path}...\")\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        main_model.load_state_dict(checkpoint['main_model_state'], strict=False)\n",
        "        memory1.load_state_dict(checkpoint['memory1_state'])\n",
        "        memory2.load_state_dict(checkpoint['memory2_state'])\n",
        "\n",
        "        # If metrics history is available, visualize it\n",
        "        if 'metrics_history' in checkpoint:\n",
        "            visualize_saved_metrics(checkpoint['metrics_history'])\n",
        "    else:\n",
        "        print(f\"Model path {model_path} not found. Exiting test.\")\n",
        "        return\n",
        "\n",
        "    # Load dataset\n",
        "    _, _, test_df = load_cot_logic_data()\n",
        "\n",
        "    # Select a few examples to test\n",
        "    test_examples = test_df.sample(3)\n",
        "\n",
        "    for i, (_, example) in enumerate(test_examples.iterrows()):\n",
        "        prompt = example['prompt']\n",
        "        reference = example['response']\n",
        "\n",
        "        print(f\"\\n--- Test Example {i+1} ---\")\n",
        "        print(f\"Prompt: {prompt[:100]}...\")\n",
        "\n",
        "        # Generate response\n",
        "        generated_bytes = generate_from_prompt(\n",
        "            main_model, memory1, memory2,\n",
        "            prompt_text=prompt,\n",
        "            max_new_tokens=512\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            generated_text = generated_bytes.decode('utf-8', errors='replace')\n",
        "            print(f\"\\nGenerated Response: {generated_text[:2000]}...\")\n",
        "\n",
        "            # Find tags in response\n",
        "            thinking_start = generated_text.find(hyperparams['thinking_tag'])\n",
        "            thinking_end = generated_text.find(hyperparams['thinking_end_tag'])\n",
        "            answer_start = generated_text.find(hyperparams['answer_tag'])\n",
        "            answer_end = generated_text.find(hyperparams['answer_end_tag'])\n",
        "\n",
        "            if thinking_start >= 0 and thinking_end > thinking_start:\n",
        "                print(\"\\nThinking Process:\")\n",
        "                print(generated_text[thinking_start:thinking_end + len(hyperparams['thinking_end_tag'])])\n",
        "\n",
        "            if answer_start >= 0 and answer_end > answer_start:\n",
        "                print(\"\\nFinal Answer:\")\n",
        "                print(generated_text[answer_start:answer_end + len(hyperparams['answer_end_tag'])])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error decoding response: {e}\")\n",
        "\n",
        "def visualize_saved_metrics(metrics_history):\n",
        "    \"\"\"Visualize metrics from a saved checkpoint.\"\"\"\n",
        "    if not metrics_history or len(metrics_history['episodes']) < 2:\n",
        "        print(\"Not enough data to visualize metrics\")\n",
        "        return\n",
        "\n",
        "    # Create figure with multiple subplots\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
        "\n",
        "    # Plot 1: Penalty Scale over episodes\n",
        "    ax1.plot(metrics_history['episodes'], metrics_history['penalty_scale'],\n",
        "            marker='o', linestyle='-', color='red')\n",
        "    ax1.set_ylabel('Penalty Scale')\n",
        "    ax1.set_title('Progressive Penalty Scaling over Episodes')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot 2: Policy Loss over episodes\n",
        "    ax2.plot(metrics_history['episodes'], metrics_history['policy_loss'],\n",
        "            marker='x', linestyle='-', color='blue')\n",
        "    ax2.set_ylabel('Policy Loss')\n",
        "    ax2.set_title('Policy Loss over Episodes')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Average Reward over episodes\n",
        "    ax3.plot(metrics_history['episodes'], metrics_history['avg_reward'],\n",
        "            marker='s', linestyle='-', color='green')\n",
        "    ax3.set_xlabel('Episode')\n",
        "    ax3.set_ylabel('Average Reward')\n",
        "    ax3.set_title('Average Reward over Episodes')\n",
        "    ax3.grid(True)\n",
        "\n",
        "    # Adjust layout and save figure\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"loaded_model_metrics_visualization.png\")\n",
        "    plt.close(fig)\n",
        "    print(\"Visualization of loaded model metrics saved to loaded_model_metrics_visualization.png\")\n",
        "\n",
        "# ==========================================\n",
        "# 11) Main Entry Point\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Progressive Reward RL Training with Memory Optimizations...\")\n",
        "    print(f\"Gradient checkpointing enabled: {hyperparams['use_gradient_checkpointing']}\")\n",
        "    print(f\"Gradient accumulation steps: {hyperparams['gradient_accumulation_steps']}\")\n",
        "    print(f\"Batch size: {hyperparams['batch_size']}\")\n",
        "    print(f\"Using dynamic quantization: {hyperparams['use_dynamic_quantization']}\")\n",
        "    print(f\"Maximum penalty scale: {hyperparams['max_penalty_scale']}\")\n",
        "\n",
        "    # Clear CUDA memory before starting\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        print_memory_stats()\n",
        "\n",
        "    try:\n",
        "        print(\"\\nRunning RL training...\")\n",
        "        train_with_progressive_rewards()\n",
        "\n",
        "        print(\"\\nTesting RL-trained model...\")\n",
        "        test_rl_model()\n",
        "\n",
        "        print(\"\\nTraining complete!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## inference V2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# ==========================================\n",
        "# 1) Define Hyperparameters for Inference\n",
        "# ==========================================\n",
        "hyperparams = {\n",
        "    'block_size': 1024,               # Sequence length for context\n",
        "    'embed_dim': 1024,                # Transformer embedding dimension\n",
        "    'n_heads': 16,                    # Number of attention heads\n",
        "    'n_layers': 24,                   # Number of Transformer blocks\n",
        "    'memory_n_layers': 8,             # Number of layers in the Memory modules\n",
        "    'vocab_size': 256,                # Fixed vocabulary size for byte tokenization\n",
        "    'bos_token': 254,                 # Beginning-of-sequence token (byte value)\n",
        "    'eos_token': 255,                 # End-of-sequence token (byte value)\n",
        "    'checkpoint_path': \"threshold_transformer_checkpoint.pt\", # Path to model checkpoint\n",
        "    'top_p': 0.6,                     # Top-p sampling parameter (0-1)\n",
        "    'system_prompt': \"\"\"IMPORTANT: Your response format should have two parts:\n",
        "    1. First, explain your thinking process in detail between <think> </think> tags.\n",
        "    2. Then, provide your final answer between <answer> </answer> tags.\n",
        "    For example: <think> Let me think about this problem carefully...\n",
        "    [detailed reasoning process] </think> <answer> [concise answer] </answer> \"\"\"\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2) Select device for inference\n",
        "# ==========================================\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \\\n",
        "         (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3) NEW: Thresholded Attention Implementation\n",
        "# ==========================================\n",
        "class ThresholdedAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        # Standard attention projections\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Attention score normalization\n",
        "        self.attn_scale = nn.Parameter(torch.ones(1) * (1.0 / math.sqrt(self.head_dim)))\n",
        "\n",
        "        # Threshold parameters for attention scores\n",
        "        self.register_buffer('score_running_mean', torch.zeros(n_heads))\n",
        "        self.register_buffer('score_running_var', torch.ones(n_heads))\n",
        "        self.score_threshold = nn.Parameter(torch.ones(1) * 0.5)\n",
        "        self.score_momentum = 0.01\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Project to queries, keys, values\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "\n",
        "        # Compute scaled attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.attn_scale  # B, H, T, T\n",
        "\n",
        "        # Apply causal mask if provided\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask, float('-inf'))\n",
        "\n",
        "        # Apply thresholding to attention scores (only in training)\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                # Compute statistics of attention scores across batch and tokens\n",
        "                # We remove the masked (very negative) values from statistics calculation\n",
        "                valid_mask = ~torch.isinf(scores)\n",
        "                if valid_mask.any():\n",
        "                    # Get head-wise mean and variance\n",
        "                    score_mean = torch.sum(scores * valid_mask, dim=(0, 2, 3)) / torch.sum(valid_mask, dim=(0, 2, 3))\n",
        "                    score_var = torch.sum(((scores - score_mean.view(1, -1, 1, 1)) ** 2) * valid_mask, dim=(0, 2, 3)) / torch.sum(valid_mask, dim=(0, 2, 3))\n",
        "\n",
        "                    # Update running statistics\n",
        "                    self.score_running_mean = (1 - self.score_momentum) * self.score_running_mean + self.score_momentum * score_mean\n",
        "                    self.score_running_var = (1 - self.score_momentum) * self.score_running_var + self.score_momentum * score_var\n",
        "\n",
        "        # Calculate adaptive threshold for attention scores\n",
        "        threshold_value = torch.sigmoid(self.score_threshold) * torch.sqrt(torch.clamp(self.score_running_var, min=1e-6))\n",
        "\n",
        "        # Create soft mask for scores (0 for values below threshold, 1 for values above)\n",
        "        # We can't use scores directly as they may have -inf values, so we'll make a mask\n",
        "        # Exclude values that are already -inf (from causal mask)\n",
        "        mask = (~torch.isinf(scores)) & (scores < threshold_value.view(1, -1, 1, 1))\n",
        "        scores = scores.masked_fill(mask, -1e4)  # Not -inf to keep gradients\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, v)  # B, H, T, D\n",
        "\n",
        "        # Reshape and project output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "    # Method to handle compatibility with original MultiheadAttention\n",
        "    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n",
        "        # Map old MHA parameters to new ThresholdedAttention parameters\n",
        "        if f\"{prefix}in_proj_weight\" in state_dict:\n",
        "            # MultiheadAttention uses a single in_proj_weight that combines q,k,v\n",
        "            in_proj_weight = state_dict.pop(f\"{prefix}in_proj_weight\")\n",
        "            in_proj_bias = state_dict.pop(f\"{prefix}in_proj_bias\", None)\n",
        "\n",
        "            # Split the in_proj_weight into q, k, v parts\n",
        "            q_weight, k_weight, v_weight = in_proj_weight.chunk(3, dim=0)\n",
        "            state_dict[f\"{prefix}q_proj.weight\"] = q_weight\n",
        "            state_dict[f\"{prefix}k_proj.weight\"] = k_weight\n",
        "            state_dict[f\"{prefix}v_proj.weight\"] = v_weight\n",
        "\n",
        "            if in_proj_bias is not None:\n",
        "                q_bias, k_bias, v_bias = in_proj_bias.chunk(3, dim=0)\n",
        "                state_dict[f\"{prefix}q_proj.bias\"] = q_bias\n",
        "                state_dict[f\"{prefix}k_proj.bias\"] = k_bias\n",
        "                state_dict[f\"{prefix}v_proj.bias\"] = v_bias\n",
        "\n",
        "        # Map out_proj parameters\n",
        "        if f\"{prefix}out_proj.weight\" in state_dict:\n",
        "            state_dict[f\"{prefix}out_proj.weight\"] = state_dict[f\"{prefix}out_proj.weight\"]\n",
        "            if f\"{prefix}out_proj.bias\" in state_dict:\n",
        "                state_dict[f\"{prefix}out_proj.bias\"] = state_dict[f\"{prefix}out_proj.bias\"]\n",
        "\n",
        "        # Call parent class method to handle the rest\n",
        "        super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n",
        "\n",
        "# ==========================================\n",
        "# 4) Model Architecture with Thresholded Attention\n",
        "# ==========================================\n",
        "class ImprovedEmergentThresholdLayer(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.norm = nn.LayerNorm(feature_dim)\n",
        "        self.register_buffer('running_mean', torch.zeros(feature_dim))\n",
        "        self.register_buffer('running_var', torch.ones(feature_dim))\n",
        "        self.adaptive_threshold = nn.Parameter(torch.ones(1) * 0.5)\n",
        "        self.momentum = 0.01\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = self.norm(x)\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                batch_mean = x_norm.mean(dim=(0, 1))\n",
        "                batch_var = x_norm.var(dim=(0, 1), unbiased=False)\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "        # More robust threshold calculation with clamping to prevent extremely small values\n",
        "        threshold = torch.sigmoid(self.adaptive_threshold) * torch.sqrt(torch.clamp(self.running_var, min=1e-6))\n",
        "\n",
        "        # Increase denominator from 0.1 to 1.0 for stability\n",
        "        gate = torch.sigmoid((torch.abs(x_norm) - threshold.view(1, 1, -1)) / 1.0)\n",
        "\n",
        "        alpha = torch.sigmoid(self.adaptive_threshold)\n",
        "\n",
        "        # Clip outputs to prevent extreme values\n",
        "        return torch.clamp(alpha * (gate * x) + (1 - alpha) * x, min=-100, max=100)\n",
        "\n",
        "class ImprovedTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        # Use ThresholdedAttention instead of nn.MultiheadAttention\n",
        "        self.attention = ThresholdedAttention(embed_dim, n_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            ImprovedEmergentThresholdLayer(4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        self.threshold1 = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "        self.threshold2 = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, E = x.size()\n",
        "        causal_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        attn_out = self.attention(x, attn_mask=causal_mask)\n",
        "        x = x + self.threshold1(attn_out)\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = x + self.threshold2(ff_out)\n",
        "        return x\n",
        "\n",
        "class ImprovedByteTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, n_heads=4, n_layers=4, block_size=128):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(self.block_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ImprovedTransformerBlock(embed_dim, n_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.final_threshold = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "        self.ln_f = nn.Linear(embed_dim, vocab_size)\n",
        "        # Learned gating parameter for combining memory outputs\n",
        "        self.gate_param = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward_with_embeddings(self, x_emb):\n",
        "        for block in self.blocks:\n",
        "            x_emb = block(x_emb)\n",
        "        x_emb = self.final_threshold(x_emb)\n",
        "        logits = self.ln_f(x_emb)\n",
        "        return logits\n",
        "\n",
        "    def forward_with_two_memory(self, x_emb, memory_module2):\n",
        "        \"\"\"\n",
        "        Extended forward pass:\n",
        "          1. Run transformer blocks on x_emb.\n",
        "          2. Apply the transformer's final threshold.\n",
        "          3. Process the result with a second memory module.\n",
        "          4. Combine the result of memory_module2 and the original x_emb using a gated combination.\n",
        "          5. Apply the final threshold on the combined representation.\n",
        "          6. Project to logits.\n",
        "        \"\"\"\n",
        "        transformer_out = x_emb\n",
        "        for block in self.blocks:\n",
        "            transformer_out = block(transformer_out)\n",
        "        transformer_out = self.final_threshold(transformer_out)\n",
        "        mem_out2 = memory_module2(transformer_out)\n",
        "        # Gated combination instead of simple addition:\n",
        "        alpha = torch.sigmoid(self.gate_param)  # Learned gating weight in [0, 1]\n",
        "        combined = alpha * mem_out2 + (1 - alpha) * x_emb\n",
        "        final_emb = self.final_threshold(combined)\n",
        "        logits = self.ln_f(final_emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.size()\n",
        "        token_emb = self.token_embedding(x)\n",
        "        positions = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "        pos_emb = self.pos_embedding(positions)\n",
        "        x_emb = token_emb + pos_emb\n",
        "        return self.forward_with_embeddings(x_emb)\n",
        "\n",
        "class MemoryModule(nn.Module):\n",
        "    def __init__(self, embed_dim, n_layers=8, expansion_factor=4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            layer = nn.Sequential(\n",
        "                nn.LayerNorm(embed_dim),\n",
        "                nn.Linear(embed_dim, embed_dim * expansion_factor),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(embed_dim * expansion_factor, embed_dim),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "        self.final_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            out = out + layer(out)\n",
        "        out = self.final_norm(out)\n",
        "        return out\n",
        "\n",
        "# ==========================================\n",
        "# 5) Model Conversion Function (for compatibility)\n",
        "# ==========================================\n",
        "def convert_original_model_to_thresholded(original_checkpoint_path, device_type=device):\n",
        "    \"\"\"\n",
        "    Function to convert a standard model checkpoint to use the thresholded attention.\n",
        "    This is a fallback if loading directly fails.\n",
        "    \"\"\"\n",
        "    print(f\"Converting original model to thresholded version: {original_checkpoint_path}\")\n",
        "\n",
        "    # Load original checkpoint\n",
        "    checkpoint = torch.load(original_checkpoint_path, map_location=device_type)\n",
        "\n",
        "    # Create new models\n",
        "    new_main_model = ImprovedByteTransformer(\n",
        "        vocab_size=hyperparams['vocab_size'],\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_heads=hyperparams['n_heads'],\n",
        "        n_layers=hyperparams['n_layers'],\n",
        "        block_size=hyperparams['block_size']\n",
        "    ).to(device_type)\n",
        "\n",
        "    new_memory1 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device_type)\n",
        "\n",
        "    new_memory2 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device_type)\n",
        "\n",
        "    # Load non-attention parts directly\n",
        "    # Embeddings, Layer Norms, Feed Forward, and Memory Modules should have identical keys\n",
        "    state_dict = checkpoint['main_model_state']\n",
        "    new_state_dict = {}\n",
        "\n",
        "    # Copy all parts that can be directly copied\n",
        "    for k, v in state_dict.items():\n",
        "        if 'attention' not in k:\n",
        "            new_state_dict[k] = v\n",
        "\n",
        "    # Load memory modules directly\n",
        "    new_memory1.load_state_dict(checkpoint['memory1_state'])\n",
        "    if 'memory2_state' in checkpoint:\n",
        "        new_memory2.load_state_dict(checkpoint['memory2_state'])\n",
        "\n",
        "    # Process attention parts\n",
        "    for layer_idx in range(hyperparams['n_layers']):\n",
        "        prefix = f\"blocks.{layer_idx}.attention.\"\n",
        "\n",
        "        # Handle MultiheadAttention parameters specially\n",
        "        if f\"{prefix}in_proj_weight\" in state_dict:\n",
        "            in_proj_weight = state_dict[f\"{prefix}in_proj_weight\"]\n",
        "            in_proj_bias = state_dict.get(f\"{prefix}in_proj_bias\", None)\n",
        "\n",
        "            # Split the weights/biases for q, k, v\n",
        "            embed_dim = hyperparams['embed_dim']\n",
        "            q_weight, k_weight, v_weight = in_proj_weight.chunk(3, dim=0)\n",
        "\n",
        "            # Set in new state dict\n",
        "            new_state_dict[f\"{prefix}q_proj.weight\"] = q_weight\n",
        "            new_state_dict[f\"{prefix}k_proj.weight\"] = k_weight\n",
        "            new_state_dict[f\"{prefix}v_proj.weight\"] = v_weight\n",
        "\n",
        "            if in_proj_bias is not None:\n",
        "                q_bias, k_bias, v_bias = in_proj_bias.chunk(3, dim=0)\n",
        "                new_state_dict[f\"{prefix}q_proj.bias\"] = q_bias\n",
        "                new_state_dict[f\"{prefix}k_proj.bias\"] = k_bias\n",
        "                new_state_dict[f\"{prefix}v_proj.bias\"] = v_bias\n",
        "\n",
        "        # Copy output projection\n",
        "        if f\"{prefix}out_proj.weight\" in state_dict:\n",
        "            new_state_dict[f\"{prefix}out_proj.weight\"] = state_dict[f\"{prefix}out_proj.weight\"]\n",
        "            if f\"{prefix}out_proj.bias\" in state_dict:\n",
        "                new_state_dict[f\"{prefix}out_proj.bias\"] = state_dict[f\"{prefix}out_proj.bias\"]\n",
        "\n",
        "    # Load the modified state dict\n",
        "    new_main_model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "    return new_main_model, new_memory1, new_memory2\n",
        "\n",
        "# ==========================================\n",
        "# 6) Inference Function with Top-p Sampling\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def generate_from_prompt(main_model, memory1, memory2, prompt_text=None, max_new_tokens=200, top_p=None):\n",
        "    if prompt_text is None:\n",
        "        prompt_text = \"Explain why the statement 'I wore my lucky socks today, and I got an A on my test, so my socks must be lucky' is a logical fallacy.\"\n",
        "\n",
        "    # Use default top_p if not specified\n",
        "    if top_p is None:\n",
        "        top_p = hyperparams['top_p']\n",
        "\n",
        "    # Apply system prompt to user prompt\n",
        "    system_prompt = hyperparams['system_prompt']\n",
        "    full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt_text}\"\n",
        "\n",
        "    # Convert prompt to bytes\n",
        "    if isinstance(full_prompt, str):\n",
        "        prompt_bytes = full_prompt.encode('utf-8')\n",
        "    elif not isinstance(full_prompt, bytes):\n",
        "        prompt_bytes = str(full_prompt).encode('utf-8')\n",
        "\n",
        "    # Set models to evaluation mode\n",
        "    main_model.eval()\n",
        "    memory1.eval()\n",
        "    memory2.eval()\n",
        "\n",
        "    # Create context from prompt\n",
        "    context = torch.tensor([b for b in prompt_bytes], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Add BOS token to start the response generation\n",
        "    bos_token = torch.tensor([[hyperparams['bos_token']]], dtype=torch.long, device=device)\n",
        "    context = torch.cat([context, bos_token], dim=1)\n",
        "\n",
        "    generated = []\n",
        "    eos_found = False\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_found:\n",
        "            break\n",
        "\n",
        "        x_cond = context[:, -hyperparams['block_size']:] if context.size(1) > hyperparams['block_size'] else context\n",
        "        B, T = x_cond.shape\n",
        "        token_emb = main_model.token_embedding(x_cond)\n",
        "        pos_emb = main_model.pos_embedding(torch.arange(T, device=x_cond.device).unsqueeze(0))\n",
        "        combined_emb = token_emb + pos_emb\n",
        "\n",
        "        mem_out1 = memory1(combined_emb)\n",
        "        logits = main_model.forward_with_two_memory(mem_out1, memory2)\n",
        "\n",
        "        # Get next token distribution with top-p (nucleus) sampling\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sort probabilities in descending order\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "        # Compute cumulative probabilities\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Find indices where cumulative probability exceeds top_p\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        # Shift to create first index (0) as False to always keep at least one token\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Create mask for indices to remove\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "\n",
        "        # Filter logits\n",
        "        filtered_logits = logits.clone()\n",
        "        filtered_logits[indices_to_remove] = -float('inf')\n",
        "\n",
        "        # Get probabilities from filtered logits\n",
        "        filtered_probs = F.softmax(filtered_logits, dim=-1)\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        next_token = torch.multinomial(filtered_probs, num_samples=1)\n",
        "        next_token_value = next_token.item()\n",
        "\n",
        "        # Check for EOS token\n",
        "        if next_token_value == hyperparams['eos_token']:\n",
        "            eos_found = True\n",
        "            break  # Immediately break out of the loop when EOS is found\n",
        "\n",
        "        # Only append non-EOS tokens to the generated sequence\n",
        "        generated.append(next_token_value)\n",
        "        context = torch.cat([context, next_token], dim=1)\n",
        "\n",
        "    # Convert generated tokens to bytes\n",
        "    result_bytes = bytes(generated)\n",
        "\n",
        "    # Clean up special tokens when returning result\n",
        "    try:\n",
        "        # Convert to string\n",
        "        response_str = result_bytes.decode('utf-8')\n",
        "        return response_str\n",
        "    except:\n",
        "        # If decoding fails, return a message\n",
        "        return \"Error: Unable to decode generated response.\"\n",
        "\n",
        "# ==========================================\n",
        "# 7) Main Inference Function\n",
        "# ==========================================\n",
        "def inference(prompt_text, max_tokens=512, top_p=None):\n",
        "    \"\"\"Generate a response for a given prompt using the pre-trained model with thresholded attention\"\"\"\n",
        "    # Use default top_p if not specified\n",
        "    if top_p is None:\n",
        "        top_p = hyperparams['top_p']\n",
        "\n",
        "    # Initialize models\n",
        "    main_model = ImprovedByteTransformer(\n",
        "        vocab_size=hyperparams['vocab_size'],\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_heads=hyperparams['n_heads'],\n",
        "        n_layers=hyperparams['n_layers'],\n",
        "        block_size=hyperparams['block_size']\n",
        "    ).to(device)\n",
        "\n",
        "    memory1 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    memory2 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    # Check if thresholded checkpoint exists\n",
        "    thresholded_checkpoint_path = hyperparams['checkpoint_path'].replace('.pt', '_thresholded.pt')\n",
        "    if os.path.exists(thresholded_checkpoint_path):\n",
        "        checkpoint_path = thresholded_checkpoint_path\n",
        "        print(f\"Using thresholded model checkpoint: {checkpoint_path}\")\n",
        "    else:\n",
        "        checkpoint_path = hyperparams['checkpoint_path']\n",
        "        print(f\"Thresholded checkpoint not found, using original: {checkpoint_path}\")\n",
        "\n",
        "    # Load pre-trained model weights\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "        # Try to load model states directly\n",
        "        try:\n",
        "            # Load main model state\n",
        "            main_model.load_state_dict(checkpoint['main_model_state'], strict=False)\n",
        "            print(\"Main model loaded with some parameters ignored (normal for model conversion)\")\n",
        "\n",
        "            # Load memory modules\n",
        "            memory1.load_state_dict(checkpoint['memory1_state'])\n",
        "            if 'memory2_state' in checkpoint:\n",
        "                memory2.load_state_dict(checkpoint['memory2_state'])\n",
        "\n",
        "            print(\"Models loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint directly: {e}\")\n",
        "            print(\"Trying model conversion approach...\")\n",
        "\n",
        "            if checkpoint_path == hyperparams['checkpoint_path']:\n",
        "                # If direct loading fails with original checkpoint, try conversion\n",
        "                main_model, memory1, memory2 = convert_original_model_to_thresholded(checkpoint_path)\n",
        "                print(\"Model converted successfully\")\n",
        "            else:\n",
        "                # If even the thresholded checkpoint fails, something is wrong\n",
        "                raise Exception(\"Failed to load both regular and thresholded checkpoints\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error loading checkpoint: {e}\")\n",
        "\n",
        "    # Generate response with top-p sampling\n",
        "    response = generate_from_prompt(\n",
        "        main_model, memory1, memory2,\n",
        "        prompt_text=prompt_text,\n",
        "        max_new_tokens=max_tokens,\n",
        "        top_p=top_p\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "# ==========================================\n",
        "# 8) Example Usage\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    import os\n",
        "    import argparse\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(description='Generate responses using thresholded attention model')\n",
        "    parser.add_argument('--prompt', type=str, default=\"Why sea is salty?\",\n",
        "                        help='Input prompt for generation')\n",
        "    parser.add_argument('--max_tokens', type=int, default=512,\n",
        "                        help='Maximum number of tokens to generate')\n",
        "    parser.add_argument('--top_p', type=float, default=hyperparams['top_p'],\n",
        "                        help='Top-p sampling parameter (0-1)')\n",
        "    parser.add_argument('--checkpoint', type=str, default=hyperparams['checkpoint_path'],\n",
        "                        help='Path to model checkpoint')\n",
        "\n",
        "    # Check if we're in a notebook environment\n",
        "    in_notebook = False\n",
        "    try:\n",
        "        get_ipython()\n",
        "        in_notebook = True\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    if in_notebook:\n",
        "        # Running in notebook, use default parameters\n",
        "        test_prompt = \"Why sea is salty?\"\n",
        "        max_tokens = 2048\n",
        "        top_p = hyperparams['top_p']\n",
        "        hyperparams['checkpoint_path'] = hyperparams['checkpoint_path']  # Keep default\n",
        "    else:\n",
        "        # Running as script, parse arguments\n",
        "        args = parser.parse_args()\n",
        "        test_prompt = args.prompt\n",
        "        max_tokens = args.max_tokens\n",
        "        top_p = args.top_p\n",
        "        hyperparams['checkpoint_path'] = args.checkpoint\n",
        "\n",
        "    print(f\"Input prompt: {test_prompt}\")\n",
        "    print(f\"Using top-p: {top_p}\")\n",
        "    print(\"\\nGenerating response...\")\n",
        "\n",
        "    try:\n",
        "        response = inference(test_prompt, max_tokens=max_tokens, top_p=top_p)\n",
        "        print(f\"\\nResponse:\\n{response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference: {e}\")"
      ],
      "metadata": {
        "id": "ob_yM3csCo4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Threshold Transformers: Adaptive Filtering in Neural Networks with Applications to Language Modeling**\n",
        "\n",
        "**Abstract**\n",
        "\n",
        "We present Threshold Transformers, a novel neural architecture that enhances standard transformer models through adaptive statistical thresholding applied across key components, including attention mechanisms and feed-forward networks. Our approach enables selective information flow based on learned activation patterns, leading to implicit sparsity without explicit pruning. We introduce an Emergent Threshold Layer (ETL) that filters activations based on statistical significance, a Thresholded Attention mechanism that focuses on salient attention connections, a dual-memory integration scheme to improve representational capacity, and a weighted training objective with KL-divergence regularization to address token frequency imbalances. Theoretically, we demonstrate that these innovations allow the model to dynamically adapt to input distributions, concentrating computational resources on statistically significant patterns while filtering noise. We show the potential of Threshold Transformers to improve both computational efficiency and representation quality in language modeling tasks.\n",
        "\n",
        "**1. Introduction**\n",
        "\n",
        "Transformer architectures have become the dominant paradigm in natural language processing (NLP) and are increasingly used in other domains due to their ability to model long-range dependencies. However, these models face challenges in computational efficiency and representation capacity, particularly when processing long sequences with varied information density. Standard transformers process all token interactions with equal weight, regardless of their relevance to the task. This uniform processing can be computationally expensive and may not optimally capture the most important information.\n",
        "\n",
        "To address these limitations, we present Threshold Transformers, which incorporate adaptive statistical thresholding mechanisms across multiple components of the transformer architecture. Our key innovations include:\n",
        "\n",
        "* **The Emergent Threshold Layer (ETL):** This layer adaptively filters activations based on their statistical significance (deviation from the mean relative to the variance), allowing the network to focus on the most informative signals.\n",
        "* **Thresholded Attention:** This mechanism selectively focuses on attention connections that exceed a learned statistical threshold, reducing computational overhead by ignoring less relevant relationships and potentially improving focus on key semantic connections.\n",
        "* **A Dual Memory Architecture:** This architecture enhances representational capacity by providing separate pathways for processing and storing different types of information, potentially allowing one pathway to preserve fine-grained token information while the other captures higher-level contextual relationships.\n",
        "* **A Weighted Cross-Entropy Loss with KL-Divergence Regularization:** This novel training objective improves token representation by accounting for frequency imbalances in the training data while preventing excessive distortion of the model's output distribution, encouraging the model to maintain a balanced understanding of the vocabulary.\n",
        "\n",
        "We provide formal mathematical descriptions of these components and analyze their theoretical properties, demonstrating how they create an implicit bias toward sparse but meaningful representations, which can lead to improved computational efficiency and potentially better generalization.\n",
        "\n",
        "**2. Background and Related Work**\n",
        "\n",
        "**2.1 Standard Transformer Architecture**\n",
        "\n",
        "The standard transformer architecture (Vaswani et al., 2017) consists of alternating multi-head attention and feed-forward layers. For a sequence of $n$ tokens with $d$-dimensional embeddings, the self-attention mechanism computes:\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "where $Q, K, V \\in \\mathbb{R}^{n \\times d}$ are query, key, and value matrices derived from the input. The multi-head attention extends this by:\n",
        "$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "where each head computes attention with separate projections:\n",
        "$$\\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)$$\n",
        "\n",
        "**2.2 Sparse and Efficient Transformers**\n",
        "\n",
        "Several approaches have been proposed to improve transformer efficiency. Sparse Transformers (Child et al., 2019) use fixed sparsity patterns in attention, such as focusing on local windows or strided patterns. Reformer (Kitaev et al., 2020) employs techniques like locality-sensitive hashing to approximate the attention mechanism and reversible layers to reduce memory footprint. Others have explored pruning (Michel et al., 2019) to remove less important connections after training and distillation (Sanh et al., 2019) to train smaller, faster models that mimic the behavior of larger ones.\n",
        "\n",
        "Our work differs from these approaches by introducing adaptive thresholding that emerges from training dynamics rather than being imposed through fixed patterns or post-training modifications. Unlike methods that enforce sparsity explicitly, Threshold Transformers learn to selectively activate components based on the statistical properties of the input data, potentially offering a more flexible and data-driven approach to efficiency.\n",
        "\n",
        "**3. Threshold Transformer Architecture**\n",
        "\n",
        "**3.1 Improved Emergent Threshold Layer (ETL)**\n",
        "\n",
        "The core of our approach is the Improved Emergent Threshold Layer (ETL), which adaptively filters activations based on their statistical significance. The ETL maintains running estimates of the mean $\\mu \\in \\mathbb{R}^d$ and variance $\\sigma^2 \\in \\mathbb{R}^d$ of normalized activations:\n",
        "$$\\hat{x} = \\text{LayerNorm}(x)$$\n",
        "$$\\mu_t = (1-\\beta)\\mu_{t-1} + \\beta \\mathbb{E}[\\hat{x}]$$\n",
        "$$\\sigma^2_t = (1-\\beta)\\sigma^2_{t-1} + \\beta \\mathbb{E}[(\\hat{x} - \\mu_t)^2]$$\n",
        "where $\\beta$ is a momentum parameter. The threshold is computed as:\n",
        "$$\\theta = \\sigma(p) \\cdot \\sqrt{\\sigma^2 + \\epsilon}$$\n",
        "where $p$ is a learnable parameter and $\\sigma(\\cdot)$ is the sigmoid function. The gating mechanism is then:\n",
        "$$g = \\sigma\\left(\\frac{|\\hat{x}| - \\theta}{\\tau}\\right)$$\n",
        "where $\\tau$ is a temperature parameter controlling the sharpness of the gating. The output of the layer is:\n",
        "$$y = \\alpha \\cdot (g \\odot x) + (1-\\alpha) \\cdot x$$\n",
        "where $\\alpha = \\sigma(p)$ controls the balance between thresholded and direct paths, and $\\odot$ denotes element-wise multiplication.\n",
        "\n",
        "**Theorem 1:** For a given distribution of activations with finite variance, as the parameter $p$ increases and $\\tau$ decreases, the ETL approximates a hard thresholding operation that passes only activations above $\\theta$ standard deviations from the mean.\n",
        "\n",
        "**Proof (Sketch):** As $\\tau \\to 0$, the sigmoid function $\\sigma\\left(\\frac{|\\hat{x}| - \\theta}{\\tau}\\right)$ approaches a step function, yielding 1 when $|\\hat{x}| > \\theta$ and 0 otherwise. Since $\\theta = \\sigma(p) \\cdot \\sqrt{\\sigma^2}$ is proportional to the standard deviation of activations, this creates a statistical filter that passes only values that deviate significantly from the mean. As $p$ increases, $\\sigma(p) \\to 1$, making the threshold closer to the full standard deviation. A more rigorous proof would involve analyzing the convergence of the sigmoid function to a step function and formally bounding the error. □\n",
        "\n",
        "**3.2 Thresholded Attention Mechanism**\n",
        "\n",
        "We extend the thresholding concept to attention scores, creating the Thresholded Attention mechanism. For a given set of query, key, and value projections, we compute attention scores as:\n",
        "$$S = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
        "For each attention head $h$, we maintain running statistics of score distributions:\n",
        "$$\\mu^S_{h,t} = (1-\\beta)\\mu^S_{h,t-1} + \\beta \\mathbb{E}[S_h]$$\n",
        "$$(\\sigma^S_h)^2_t = (1-\\beta)(\\sigma^S_h)^2_{t-1} + \\beta \\mathbb{E}[(S_h - \\mu^S_{h,t})^2]$$\n",
        "We compute a head-specific threshold:\n",
        "$$\\theta^S_h = \\sigma(p^S) \\cdot \\sqrt{(\\sigma^S_h)^2 + \\epsilon}$$\n",
        "where $p^S$ is a learnable parameter shared across attention heads. We create a mask $M$ for scores:\n",
        "$$M_{ij} = \\begin{cases}\n",
        "1 & \\text{if } S_{ij} < \\theta^S_h \\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "The thresholded attention score matrix becomes:\n",
        "$$\\hat{S} = S \\odot (1-M) + (-\\infty) \\odot M$$\n",
        "This is then passed through softmax to obtain the attention weights:\n",
        "$$A = \\text{softmax}(\\hat{S})$$\n",
        "\n",
        "**Theorem 2:** Thresholded Attention reduces the effective attention span by eliminating connections with scores below the adaptive threshold, creating an emergent sparse attention pattern that depends on the input distribution.\n",
        "\n",
        "**Proof (Sketch):** The mask $M$ sets scores below the head-specific threshold $\\theta^S_h$ to $-\\infty$. The softmax operation then assigns exponentially smaller weights to these masked scores, effectively eliminating their contribution to the attention mechanism. The expected sparsity ratio is the probability that an attention score $S_{ij}$ falls below the threshold $\\theta^S_h$, i.e., $\\mathbb{P}(S_{ij} < \\theta^S_h)$. If we assume a Gaussian distribution for the attention scores, this probability can be related to the cumulative distribution function (CDF) of the standard normal distribution. As $\\theta^S_h$ increases (driven by the learned parameter $p^S$), the sparsity ratio increases, leading to a reduction in the effective attention span. A more formal proof would require assumptions about the distribution of attention scores and analysis of the impact of the threshold on the attention weights. □\n",
        "\n",
        "**3.3 Dual Memory Architecture**\n",
        "\n",
        "Our architecture incorporates a main transformer model augmented with two memory modules. Each memory module consists of feed-forward layers with residual connections, similar to the post-attention and post-feed-forward blocks in a standard transformer layer (He et al., 2016):\n",
        "$$\\text{Memory}(x) = \\text{LN}\\left(x + \\sum_{i=1}^L \\text{FF}_i(x)\\right)$$\n",
        "where $\\text{FF}_i(x) = W^2_i \\cdot \\text{GELU}(W^1_i \\cdot \\text{LN}(x))$ and LN denotes Layer Normalization (Ba et al., 2016).\n",
        "\n",
        "The complete forward pass through the model is:\n",
        "$$e = \\text{TokenEmbed}(x) + \\text{PosEmbed}(x)$$\n",
        "$$m_1 = \\text{Memory}_1(e)$$\n",
        "$$t = \\text{Transformer}(m_1)$$\n",
        "$$m_2 = \\text{Memory}_2(t)$$\n",
        "$$\\gamma = \\sigma(g)$$\n",
        "$$o = \\gamma \\cdot m_2 + (1-\\gamma) \\cdot e$$\n",
        "$$y = \\text{Threshold}(o)$$\n",
        "$$\\text{logits} = W^{\\text{out}} \\cdot y$$\n",
        "where $g$ is a learnable parameter controlling the gating between memory outputs, and `Threshold` represents a final application of the ETL. We hypothesize that $\\text{Memory}_1$ might help in initial processing of the input embeddings, while $\\text{Memory}_2$, operating on the output of the main transformer, could further refine the contextualized representations. The direct pathway from input embeddings ($e$) to the output allows the model to retain some low-level token information, while the pathway through the transformer and memory modules captures higher-order contextual relationships.\n",
        "\n",
        "**Proposition 1:** The dual memory architecture creates two distinct information pathways: (1) a direct pathway from input embeddings to output that preserves low-level token information, and (2) a transformed pathway that captures higher-order contextual relationships. The gating parameter $\\gamma$ learns to balance these pathways based on the task requirements.\n",
        "\n",
        "**3.4 Weighted Cross-Entropy with KL Regularization**\n",
        "\n",
        "We propose a novel training objective that accounts for token frequency imbalance while preventing excessive distortion of the model's distribution. For token frequencies $p_i$ in the training corpus, we compute weights:\n",
        "$$w_i = \\min\\left(\\left(\\frac{1}{p_i}\\right)^{\\alpha}, w_{\\max}\\right)$$\n",
        "where $\\alpha \\in [0,1]$ controls the weighting strength and $w_{\\max}$ is a cap to prevent numerical instability. This weighting scheme assigns higher weights to less frequent tokens, encouraging the model to learn better representations for them.\n",
        "\n",
        "The weighted cross-entropy loss is:\n",
        "$$\\mathcal{L}_{\\text{WCE}}(y, \\hat{y}) = -\\sum_i w_i y_i \\log(\\hat{y}_i)$$\n",
        "To prevent excessive deviation from the unweighted distribution, which could lead to overfitting or a degradation in the representation of frequent tokens, we add a KL-divergence regularization term:\n",
        "$$\\mathcal{L}{\\text{KL}}(\\hat{y}, \\tilde{y}) = D{\\text{KL}}(\\hat{y} || \\tilde{y})$$\n",
        "where $\\tilde{y}$ is the model's output distribution with unweighted training (in practice, this might be approximated or a target distribution from a pre-trained model).\n",
        "\n",
        "The final loss becomes:\n",
        "$$\\mathcal{L} = \\mathcal{L}{\\text{WCE}}(y, \\hat{y}) + \\lambda \\mathcal{L}{\\text{KL}}(\\hat{y}, \\tilde{y})$$\n",
        "In practice, we approximate the KL term using the difference between weighted and unweighted cross-entropy losses:\n",
        "$$\\mathcal{L} \\approx \\mathcal{L}{\\text{WCE}}(y, \\hat{y}) + \\lambda|\\mathcal{L}{\\text{WCE}}(y, \\hat{y}) - \\mathcal{L}_{\\text{CE}}(y, \\hat{y})|$$\n",
        "This approximation provides a computationally efficient way to encourage the model to stay close to the unweighted distribution while still benefiting from the weighted loss.\n",
        "\n",
        "**4. Theoretical Analysis**\n",
        "\n",
        "**4.1 Emergent Sparsity and Information Flow**\n",
        "\n",
        "The Threshold Transformer architecture induces an emergent form of sparsity without requiring explicit sparse attention patterns or pruning.\n",
        "\n",
        "**Theorem 3:** Under the assumptions of (i) normalized activation distributions (approximately standard normal after LayerNorm) and (ii) learnable threshold parameters converging to optimal values, the expected proportion of activations filtered by the ETL converges to $2\\Phi(-\\sigma(p^*))$, where $\\Phi$ is the CDF of the standard normal distribution and $p^*$ is the converged value of the threshold parameter.\n",
        "\n",
        "**Proof:** For normalized activations following approximately a standard normal distribution, the probability of an activation having an absolute value below the threshold $\\theta = \\sigma(p) \\cdot \\sqrt{\\sigma^2}$ (where $\\sigma^2 \\approx 1$ for normalized activations) is $\\mathbb{P}(|\\hat{x}| < \\sigma(p)) = \\Phi(\\sigma(p)) - \\Phi(-\\sigma(p)) = 2\\Phi(\\sigma(p)) - 1$. The proportion filtered is the probability that $|\\hat{x}| \\le \\theta$, which is $1 - \\mathbb{P}(|\\hat{x}| > \\theta) = 1 - (\\mathbb{P}(\\hat{x} > \\theta) + \\mathbb{P}(\\hat{x} < -\\theta)) = 1 - (1 - \\Phi(\\theta) + \\Phi(-\\theta)) = 2\\Phi(-\\theta) = 2\\Phi(-\\sigma(p))$. At convergence, where $p$ reaches its optimal value $p^*$, the expected proportion of filtered activations is $2\\Phi(-\\sigma(p^*))$. □\n",
        "\n",
        "**4.2 Computational Efficiency Analysis**\n",
        "\n",
        "The thresholding mechanisms provide computational benefits by reducing the effective operations needed.\n",
        "\n",
        "**Proposition 2:** Assuming hardware support for sparse operations, the computational complexity of the Thresholded Attention mechanism improves from $O(n^2d)$ to $O(sn^2d)$, where $s$ is the sparsity ratio (the proportion of attention scores below the threshold, leading to zero weights) determined by the learned threshold.\n",
        "\n",
        "In practice, current hardware often doesn't fully exploit sparse computation efficiently. However, future specialized hardware designed to handle sparse matrix multiplications could leverage this property for significant efficiency gains in Thresholded Attention layers. Similarly, the ETL reduces the number of activations that are passed through subsequent layers, potentially leading to further computational savings.\n",
        "\n",
        "**4.3 Representational Power and Capacity**\n",
        "\n",
        "**Theorem 4:** The dual memory architecture with thresholding increases the model's effective capacity by a factor related to the gating parameter $\\gamma$ compared to a standard transformer of the same size.\n",
        "\n",
        "**Proof Sketch:** The gating parameter $\\gamma$ creates an interpolation between two distinct processing pathways: the direct embedding pathway and the transformed pathway through the transformer and memory modules. This can be viewed as a form of soft mixture of experts (Shazeer et al., 2017; Lin et al., 2022), where the model can choose to rely more on one pathway over the other based on the input. When $\\gamma$ is optimally learned to be neither 0 nor 1, both pathways contribute to the final output, effectively increasing the model's ability to represent complex functions compared to a single pathway model. The exact factor of increase would depend on the learned value of $\\gamma$ and the effective capacity of each pathway. A simplified view suggests an increase proportional to $(1 + \\gamma)$ as it represents a weighted sum of the original embedding space and the transformed space. A more rigorous analysis would involve considering the Vapnik-Chervonenkis dimension or other measures of model capacity. □\n",
        "\n",
        "**5. Implementation Details**\n",
        "\n",
        "The Threshold Transformer is implemented with the following hyperparameters. The choice of these hyperparameters was guided by empirical observations and standard practices in transformer-based language modeling.\n",
        "\n",
        "* Embedding dimension: 1024\n",
        "* Number of attention heads: 16\n",
        "* Number of transformer layers: 24\n",
        "* Number of memory layers: 8 (4 layers in each of Memory\\_1 and Memory\\_2)\n",
        "* Token vocabulary size: 256 (byte-level tokenization)\n",
        "* Block size (sequence length): 1024\n",
        "* Minimum frequency threshold: 1e-5\n",
        "* Maximum weight cap: 10.0\n",
        "* KL regularization weight (λ): 0.1\n",
        "* Weighting exponent (α): 0.5\n",
        "* Momentum parameter (β): 0.9\n",
        "* Temperature parameter (τ): 0.1\n",
        "\n",
        "**Algorithm 1: Forward Pass with Thresholded Attention**\n",
        "\n",
        "Input: Token sequence x\n",
        "Output: Next token probability distribution\n",
        "\n",
        "1.  $e \\leftarrow \\text{TokenEmbed}(x) + \\text{PosEmbed}(x)$\n",
        "2.  $m_1 \\leftarrow \\text{Memory}_1(e)$\n",
        "3.  $t \\leftarrow m_1$\n",
        "4.  // Apply transformer blocks with thresholded attention\n",
        "5.  for $i = 1$ to $num\\_layers$ do\n",
        "6.      // Multi-Head Self-Attention with Thresholding\n",
        "7.      $Q, K, V \\leftarrow \\text{ProjectQKV}(t)$ // Linear projections for each head\n",
        "8.      $S \\leftarrow \\text{MultiHeadAttentionScore}(Q, K)$ // Concatenate scores from all heads\n",
        "9.      // Apply threshold to scores for each head\n",
        "10.     for each head $h$:\n",
        "11.         $S_h \\leftarrow S[\\text{head}_h]$\n",
        "12.         $\\text{update\\_statistics}(S_h)$ // Update running mean and variance for head $h$\n",
        "13.         $\\theta^S_h \\leftarrow \\sigma(p^S) \\cdot \\sqrt{(\\sigma^S_h)^2 + \\epsilon}$\n",
        "14.         $M_h \\leftarrow (S_h < \\theta^S_h)$\n",
        "15.         $\\hat{S}_h \\leftarrow S_h \\odot (1-M_h) + (-\\infty) \\odot M_h$\n",
        "16.     end for\n",
        "17.     $\\hat{S} \\leftarrow \\text{Concatenate}(\\hat{S}_1, ..., \\hat{S}_h)$\n",
        "18.     $A \\leftarrow \\text{softmax}(\\hat{S})$\n",
        "19.     $h \\leftarrow A V_{concatenated}$ // Apply attention weights to concatenated value projections\n",
        "20.     $t \\leftarrow \\text{LayerNorm}(t + h)$\n",
        "21.     // Feed-Forward Network with Thresholding\n",
        "22.     $f \\leftarrow \\text{FeedForward}(t)$\n",
        "23.     $t \\leftarrow \\text{LayerNorm}(t + \\text{ThresholdLayer}(f))$ // Apply ETL after Feed-Forward\n",
        "24. end for\n",
        "25. $m_2 \\leftarrow \\text{Memory}_2(t)$\n",
        "26. $\\gamma \\leftarrow \\sigma(g)$\n",
        "27. $o \\leftarrow \\gamma \\cdot m_2 + (1-\\gamma) \\cdot e$\n",
        "28. $y \\leftarrow \\text{ThresholdLayer}(o)$ // Final application of ETL\n",
        "29. return $\\text{softmax}(W^{out} \\cdot y)$\n",
        "\n",
        "**Note:** The `update_statistics(S)` function would update the running mean and variance for the given input tensor. The `ThresholdLayer(x)` function would apply the Emergent Threshold Layer as described in Section 3.1. The `MultiHeadAttentionScore` function would compute the scaled dot-product attention scores for all heads before thresholding. The projection of Q, K, and V would typically be head-specific.\n",
        "\n",
        "**6. Discussion and Implications**\n",
        "\n",
        "The Threshold Transformer introduces several innovations with important implications for neural network design:\n",
        "\n",
        "* **Adaptive Computation:** Unlike models with fixed sparsity patterns, our approach learns to allocate computational resources based on the statistical properties of inputs. This adaptivity allows the model to be more efficient when processing sequences with varying information density, potentially leading to significant speedups without sacrificing performance on informative segments.\n",
        "* **Statistical Learning Bias:** The thresholding mechanism creates an inductive bias toward focusing on statistically significant patterns in the data while filtering out noise. This bias might improve generalization by preventing the model from overfitting to spurious correlations in the training data and encouraging it to learn more robust representations.\n",
        "* **Architectural Flexibility:** The threshold parameters provide a continuous way to trade off between computation and accuracy. By adjusting these parameters (or their learning rates), models can be adapted to different computational constraints without requiring complete retraining, offering a form of dynamic resource allocation.\n",
        "* **Bridging Disciplines:** The mathematical formulation of threshold mechanisms, drawing inspiration from statistical signal processing, suggests fruitful directions for future research at the intersection of traditional neural networks and statistical methods. This could lead to new ways of incorporating statistical principles into neural network design.\n",
        "\n",
        "**7. Conclusion**\n",
        "\n",
        "We have presented Threshold Transformers, a novel neural architecture that incorporates adaptive statistical thresholding mechanisms across multiple components. Our mathematical analysis demonstrates that these threshold mechanisms create an emergent form of sparsity that adapts to input distributions, potentially improving both computational efficiency and representation quality.\n",
        "\n",
        "The dual memory architecture further enhances the model's capacity to maintain multiple types of information through separate pathways with learned gating. The weighted cross-entropy loss with KL-divergence regularization provides a principled approach to handling token frequency imbalance without excessive distribution distortion, leading to improved learning of rare tokens.\n",
        "\n",
        "Future work will explore applications of these threshold mechanisms to other neural architectures and domains beyond natural language processing, such as computer vision and time series analysis. We also plan to investigate specialized hardware implementations that could fully leverage the sparsity properties induced by the thresholding mechanisms to achieve significant efficiency gains. Further empirical evaluation on a wider range of language modeling benchmarks and ablation studies to assess the contribution of each component will also be crucial.\n",
        "\n",
        "**References**\n",
        "\n",
        "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems.\n",
        "\n",
        "Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.\n",
        "\n",
        "Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). Reformer: The efficient transformer. In International Conference on Learning Representations.\n",
        "\n",
        "Michel, P., Levy, O., & Neubig, G. (2019). Are sixteen heads really better than one? In Advances in Neural Information Processing Systems.\n",
        "\n",
        "Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n",
        "\n",
        "He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\n",
        "\n",
        "Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n",
        "\n",
        "Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.\n",
        "\n",
        "Hendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415.\n",
        "\n",
        "Lin, T., Jin, S., & Ghahramani, Z. (2022). Adaptive representation plasticity for continual learning. Advances in Neural Information Processing Systems, 35.\n",
        "\n",
        "This updated version incorporates the feedback and provides more detail in certain areas. Remember that a complete paper would require experimental results and a more thorough analysis of those results."
      ],
      "metadata": {
        "id": "fN138mdWEzy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## pretraining V2.1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ==========================================\n",
        "# 1) Hyperparameters (modified for new dataset)\n",
        "# ==========================================\n",
        "hyperparams = {\n",
        "    # Model Architecture\n",
        "    'block_size': 1024,               # Sequence length for context\n",
        "    'batch_size': 2,                  # Batch size\n",
        "    'embed_dim': 1024,                # Transformer embedding dimension\n",
        "    'n_heads': 16,                    # Number of attention heads\n",
        "    'n_layers': 24,                   # Number of Transformer blocks\n",
        "    'memory_n_layers': 8,             # Number of layers in the original MemoryModule\n",
        "    'vocab_size': 256,                # Fixed vocabulary size for byte tokenization\n",
        "\n",
        "    # Training Parameters\n",
        "    'num_epochs': 100,                # Number of epochs\n",
        "    'steps_per_epoch': 1000,          # Steps per epoch\n",
        "    'eval_interval': 200,             # Steps between loss evaluations\n",
        "    'eval_iters': 100,                # Iterations to average validation loss\n",
        "    'accumulation_steps': 8,          # Number of steps to accumulate gradients over\n",
        "    'validation_split': 0.1,          # Fraction of data to use for validation\n",
        "    'sample_size': 1000000,           # Number of samples to use from dataset\n",
        "\n",
        "    # Weighted Loss Parameters\n",
        "    'use_weighted_loss': True,        # Whether to use weighted cross-entropy\n",
        "    'alpha': 0.5,                     # Alpha parameter for (1/p_i)^alpha weighting\n",
        "    'kl_lambda': 0.1,                 # Lambda for KL divergence regularization\n",
        "    'min_freq': 1e-5,                 # Minimum frequency to avoid division by zero\n",
        "    'max_weight': 10.0,               # Maximum weight cap to prevent instability\n",
        "\n",
        "    # Generation Parameters\n",
        "    'generate_num_tokens': 2048,      # Number of tokens to generate after each epoch\n",
        "    'top_p': 0.8,                     # Top-p (nucleus) sampling parameter\n",
        "    'start_prompt': \"Explain why the statement 'I wore my lucky socks today, and I got an A on my test, so my socks must be lucky' is a logical fallacy.\",\n",
        "\n",
        "    # Special Tokens & Tags\n",
        "    'thinking_tag': \"<think>\",        # Opening tag for thinking process\n",
        "    'thinking_end_tag': \"</think>\",   # Closing tag for thinking process\n",
        "    'answer_tag': \"<answer>\",         # Opening tag for final answer\n",
        "    'answer_end_tag': \"</answer>\",    # Closing tag for final answer\n",
        "    'bos_token': 254,                 # Beginning-of-sequence token (byte value)\n",
        "    'eos_token': 255,                 # End-of-sequence token (byte value)\n",
        "\n",
        "    # File Paths & Modes\n",
        "    'checkpoint_path': \"threshold_transformer_checkpoint.pt\",  # Updated checkpoint name\n",
        "    'dataset_path': \"hf://datasets/applied-ai-018/pretraining_v1-omega_books/CC-MAIN-2013-20/train-*.parquet\",\n",
        "    'mode': 'pretrain',               # Force pretrain mode\n",
        "    'continue_training': True,        # Whether to continue training from a checkpoint\n",
        "    'system_prompt': \"\"\"just think before answer.\"\"\"\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 1.1) Select device\n",
        "# ==========================================\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \\\n",
        "         (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==========================================\n",
        "# 1.2) Data Loading and Preprocessing for Omega Books Dataset\n",
        "# ==========================================\n",
        "def load_omega_books_data_as_bytes():\n",
        "    \"\"\"\n",
        "    Load Omega Books dataset and convert text to bytes for byte-level tokenization.\n",
        "    Returns raw bytes and tensor versions of train and validation data.\n",
        "    \"\"\"\n",
        "    print(\"Loading Omega Books dataset as bytes...\")\n",
        "\n",
        "    try:\n",
        "        # Load data from Hugging Face using Dask for distributed processing\n",
        "        ddf = dd.read_parquet(hyperparams['dataset_path'])\n",
        "\n",
        "        # Basic data analysis\n",
        "        print(f\"Dataset partitions: {ddf.npartitions}\")\n",
        "        print(f\"Column names: {ddf.columns.tolist()}\")\n",
        "\n",
        "        # Get a sample to understand the data structure\n",
        "        sample = ddf.head(5)\n",
        "        print(\"\\nFirst 5 rows (sample):\")\n",
        "        print(sample)\n",
        "\n",
        "        # Check for missing values in sample\n",
        "        print(\"\\nMissing values in sample:\")\n",
        "        print(sample.isnull().sum())\n",
        "\n",
        "        # Try to identify content columns based on common names in text datasets\n",
        "        columns = ddf.columns.tolist()\n",
        "        content_cols = [col for col in columns if col.lower() in ['text', 'content', 'body', 'document']]\n",
        "\n",
        "        if not content_cols:\n",
        "            print(\"Could not identify text content columns, using first column\")\n",
        "            content_col = columns[0]\n",
        "        else:\n",
        "            content_col = content_cols[0]\n",
        "\n",
        "        print(f\"Using '{content_col}' as content column\")\n",
        "\n",
        "        # Process in chunks - use the sample_size parameter to control memory usage\n",
        "        sample_size = hyperparams.get('sample_size', 500000)\n",
        "\n",
        "        # Try with Dask first for distributed processing\n",
        "        try:\n",
        "            train_ddf = ddf.sample(frac=(1-hyperparams['validation_split']), random_state=42)\n",
        "            val_ddf = ddf.sample(frac=hyperparams['validation_split'], random_state=42)\n",
        "\n",
        "            # Compute to convert to pandas (with limit to avoid memory issues)\n",
        "            train_sample = train_ddf.head(sample_size)\n",
        "            val_sample = val_ddf.head(int(sample_size * hyperparams['validation_split']))\n",
        "\n",
        "            print(f\"Training sample size: {len(train_sample)}\")\n",
        "            print(f\"Validation sample size: {len(val_sample)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Dask sampling failed with error: {e}\")\n",
        "            print(\"Falling back to Hugging Face datasets approach\")\n",
        "\n",
        "            # Fall back to original approach using Hugging Face datasets\n",
        "            dataset = load_dataset(\"applied-ai-018/pretraining_v1-omega_books\", \"CC-MAIN-2013-20\", split=\"train\")\n",
        "            df = dataset.select(range(sample_size)).to_pandas()\n",
        "\n",
        "            # Clean data\n",
        "            df = df.dropna(subset=[content_col])\n",
        "            df = df[df[content_col].str.strip() != '']\n",
        "\n",
        "            # Split\n",
        "            train_sample, val_sample = train_test_split(\n",
        "                df, test_size=hyperparams['validation_split'], random_state=42\n",
        "            )\n",
        "\n",
        "            print(f\"Training examples (fallback): {len(train_sample)}\")\n",
        "            print(f\"Validation examples (fallback): {len(val_sample)}\")\n",
        "\n",
        "        # Convert data to bytes for byte-level tokenization\n",
        "        train_bytes = []\n",
        "        for _, row in train_sample.iterrows():\n",
        "            if content_col in row and pd.notna(row[content_col]) and isinstance(row[content_col], str):\n",
        "                byte_data = row[content_col].encode('utf-8')\n",
        "                train_bytes.extend(byte_data)\n",
        "\n",
        "        val_bytes = []\n",
        "        for _, row in val_sample.iterrows():\n",
        "            if content_col in row and pd.notna(row[content_col]) and isinstance(row[content_col], str):\n",
        "                byte_data = row[content_col].encode('utf-8')\n",
        "                val_bytes.extend(byte_data)\n",
        "\n",
        "        print(f\"Training bytes: {len(train_bytes)}\")\n",
        "        print(f\"Validation bytes: {len(val_bytes)}\")\n",
        "\n",
        "        # Convert bytes to tensors for easier processing in the model\n",
        "        train_bytes_tensor = torch.tensor(train_bytes, dtype=torch.long)\n",
        "        val_bytes_tensor = torch.tensor(val_bytes, dtype=torch.long)\n",
        "\n",
        "        return train_bytes, val_bytes, train_bytes_tensor, val_bytes_tensor\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        raise RuntimeError(f\"Unable to load the Omega Books dataset: {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# 1.2.1) Legacy Data Loading Function (kept for compatibility)\n",
        "# ==========================================\n",
        "def load_omega_books_data():\n",
        "    \"\"\"Original data loading function kept for compatibility.\"\"\"\n",
        "    print(\"Loading Omega Books dataset (legacy method)...\")\n",
        "\n",
        "    try:\n",
        "        # Load dataset using datasets library\n",
        "        dataset = load_dataset(\"applied-ai-018/pretraining_v1-omega_books\", \"CC-MAIN-2013-20\", split=\"train\")\n",
        "        print(\"Dataset loaded using datasets library\")\n",
        "\n",
        "        # Convert to pandas DataFrame and sample a portion for manageable training\n",
        "        sample_size = min(hyperparams.get('sample_size', 50000), len(dataset))\n",
        "        df = dataset.select(range(sample_size)).to_pandas()\n",
        "        print(f\"Sampled {sample_size} examples from dataset\")\n",
        "\n",
        "        # Clean and preprocess data\n",
        "        df = df.dropna(subset=['text'])\n",
        "        df = df[df['text'].str.strip() != '']\n",
        "\n",
        "        # Split into train/validation/test sets (80/10/10)\n",
        "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "        print(f\"Training examples: {len(train_df)}\")\n",
        "        print(f\"Validation examples: {len(val_df)}\")\n",
        "        print(f\"Test examples: {len(test_df)}\")\n",
        "\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        raise RuntimeError(f\"Unable to load the Omega Books dataset: {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# 1.2.2) Prepare Batches from Byte Data\n",
        "# ==========================================\n",
        "def prepare_byte_batches(byte_tensor, block_size, batch_size, device):\n",
        "    \"\"\"\n",
        "    Prepare batches directly from byte tensor for more efficient processing.\n",
        "\n",
        "    Args:\n",
        "        byte_tensor: Tensor of bytes\n",
        "        block_size: Context length for sequence\n",
        "        batch_size: Number of sequences per batch\n",
        "        device: Torch device to send tensors to\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (input_batch, target_batch) tensors\n",
        "    \"\"\"\n",
        "    # Get total possible starting positions\n",
        "    n = len(byte_tensor) - block_size\n",
        "    if n <= 0:\n",
        "        raise ValueError(f\"Byte data length ({len(byte_tensor)}) must be greater than block_size ({block_size})\")\n",
        "\n",
        "    # Randomly select starting positions\n",
        "    start_indices = torch.randint(0, n, (batch_size,))\n",
        "\n",
        "    # Create input sequences\n",
        "    x = torch.stack([byte_tensor[i:i+block_size] for i in start_indices])\n",
        "\n",
        "    # Create target sequences (shifted by 1)\n",
        "    y = torch.stack([byte_tensor[i+1:i+block_size+1] for i in start_indices])\n",
        "\n",
        "    # Send to device\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# ==========================================\n",
        "# 1.2.3) Legacy Batch Preparation for Pre-training (kept for compatibility)\n",
        "# ==========================================\n",
        "def prepare_pretraining_batches_from_omega(data_df, block_size=1024):\n",
        "    \"\"\"Create pre-training batches from Omega Books corpus as continuous text for next-token prediction.\"\"\"\n",
        "\n",
        "    batch_indices = torch.randint(0, len(data_df), (hyperparams['batch_size'],))\n",
        "    batch_examples = data_df.iloc[batch_indices]\n",
        "\n",
        "    sequences = []\n",
        "\n",
        "    for _, row in batch_examples.iterrows():\n",
        "        # Get text content from the dataset\n",
        "        text = row['text']\n",
        "\n",
        "        # Make sure we have valid text\n",
        "        if not isinstance(text, str) or text.strip() == '':\n",
        "            # Skip invalid examples\n",
        "            continue\n",
        "\n",
        "        # Add system prompt occasionally to help model learn the prompt format (20% chance)\n",
        "        if torch.rand(1).item() < 0.2:\n",
        "            system_prompt = hyperparams['system_prompt']\n",
        "            # Randomly create a thinking/answer structure\n",
        "            thinking = \"Let me think about this carefully... This requires analyzing the logical structure.\"\n",
        "            answer = \"This statement exhibits the post hoc fallacy, assuming correlation implies causation.\"\n",
        "            formatted_text = f\"{system_prompt}\\n\\nQuestion: {text}\\n\\n{chr(hyperparams['bos_token'])}<think>{thinking}</think><answer>{answer}</answer>{chr(hyperparams['eos_token'])}\"\n",
        "        else:\n",
        "            # Just use the raw text for general knowledge learning\n",
        "            formatted_text = text\n",
        "\n",
        "        # Convert to byte sequence\n",
        "        byte_seq = [b for b in formatted_text.encode('utf-8')]\n",
        "\n",
        "        # Truncate or pad to block_size\n",
        "        if len(byte_seq) > block_size:\n",
        "            # Random offset for diverse training\n",
        "            start_idx = torch.randint(0, len(byte_seq) - block_size, (1,)).item()\n",
        "            byte_seq = byte_seq[start_idx:start_idx + block_size]\n",
        "        else:\n",
        "            byte_seq = byte_seq + [0] * (block_size - len(byte_seq))\n",
        "\n",
        "        sequences.append(byte_seq)\n",
        "\n",
        "    # Make sure we have at least one valid sequence\n",
        "    if not sequences:\n",
        "        # Create a dummy sequence if none were valid\n",
        "        dummy_text = \"This is a placeholder text.\"\n",
        "        byte_seq = [b for b in dummy_text.encode('utf-8')]\n",
        "        byte_seq = byte_seq + [0] * (block_size - len(byte_seq))\n",
        "        sequences.append(byte_seq)\n",
        "\n",
        "    # Convert to tensor\n",
        "    x = torch.tensor(sequences, dtype=torch.long).to(device)\n",
        "\n",
        "    # Create targets by shifting input by 1 position\n",
        "    y = torch.full_like(x, 0)\n",
        "    y[:, :-1] = x[:, 1:].clone()\n",
        "    y[:, -1] = 0  # Last position predicts padding\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# ==========================================\n",
        "# 1.2.4) Token Frequency Analysis for Weighted Loss\n",
        "# ==========================================\n",
        "def compute_token_frequencies(byte_tensor, vocab_size=256):\n",
        "    \"\"\"Compute the frequency of each token in the byte data.\"\"\"\n",
        "    print(\"Computing token frequencies for weighted loss...\")\n",
        "\n",
        "    # Initialize frequency counter for all possible byte values\n",
        "    token_counts = torch.zeros(vocab_size, device=byte_tensor.device)\n",
        "\n",
        "    # Use a subset of data if tensor is too large\n",
        "    if len(byte_tensor) > 1_000_000:\n",
        "        print(f\"Using a 1M sample from {len(byte_tensor)} bytes for frequency analysis\")\n",
        "        indices = torch.randint(0, len(byte_tensor), (1_000_000,))\n",
        "        byte_sample = byte_tensor[indices]\n",
        "    else:\n",
        "        byte_sample = byte_tensor\n",
        "\n",
        "    # Count byte frequencies using torch operations\n",
        "    for b in range(vocab_size):\n",
        "        token_counts[b] = (byte_sample == b).sum().float()\n",
        "\n",
        "    # Calculate frequencies\n",
        "    total_tokens = token_counts.sum()\n",
        "    if total_tokens > 0:\n",
        "        token_frequencies = token_counts / total_tokens\n",
        "    else:\n",
        "        token_frequencies = torch.ones(vocab_size, device=byte_tensor.device) / vocab_size\n",
        "\n",
        "    # Apply minimum frequency to avoid division by zero\n",
        "    token_frequencies = torch.clamp(token_frequencies, min=hyperparams['min_freq'])\n",
        "\n",
        "    print(f\"Token frequency analysis complete. Most common token frequency: {token_frequencies.max().item():.6f}\")\n",
        "    return token_frequencies\n",
        "\n",
        "# ==========================================\n",
        "# 1.2.5) Compute Weights from Token Frequencies\n",
        "# ==========================================\n",
        "def compute_weights_from_frequencies(token_frequencies, alpha=0.5):\n",
        "    \"\"\"Compute weights using the formula: w_i = (1/p_i)^alpha with constraints.\"\"\"\n",
        "    weights = (1.0 / token_frequencies) ** alpha\n",
        "\n",
        "    # Cap maximum weight to prevent instability\n",
        "    weights = torch.clamp(weights, max=hyperparams['max_weight'])\n",
        "\n",
        "    # Normalize weights to have reasonable scale\n",
        "    if weights.sum() > 0:\n",
        "        weights = weights * (len(weights) / weights.sum())\n",
        "\n",
        "    return weights\n",
        "\n",
        "# ==========================================\n",
        "# 2) Improved Emergent Threshold Layer with Numerical Stability\n",
        "# ==========================================\n",
        "class ImprovedEmergentThresholdLayer(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.norm = nn.LayerNorm(feature_dim)\n",
        "        self.register_buffer('running_mean', torch.zeros(feature_dim))\n",
        "        self.register_buffer('running_var', torch.ones(feature_dim))\n",
        "        self.adaptive_threshold = nn.Parameter(torch.ones(1) * 0.5)\n",
        "        self.momentum = 0.01\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = self.norm(x)\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                batch_mean = x_norm.mean(dim=(0, 1))\n",
        "                batch_var = x_norm.var(dim=(0, 1), unbiased=False)\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "        # More robust threshold calculation with clamping to prevent extremely small values\n",
        "        threshold = torch.sigmoid(self.adaptive_threshold) * torch.sqrt(torch.clamp(self.running_var, min=1e-6))\n",
        "\n",
        "        # Increase denominator from 0.1 to 1.0 for stability\n",
        "        gate = torch.sigmoid((torch.abs(x_norm) - threshold.view(1, 1, -1)) / 1.0)\n",
        "\n",
        "        alpha = torch.sigmoid(self.adaptive_threshold)\n",
        "\n",
        "        # Clip outputs to prevent extreme values\n",
        "        return torch.clamp(alpha * (gate * x) + (1 - alpha) * x, min=-100, max=100)\n",
        "\n",
        "# ==========================================\n",
        "# 3) Thresholded Attention Mechanism\n",
        "# ==========================================\n",
        "class ThresholdedAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        # Standard attention projections\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Attention score normalization\n",
        "        self.attn_scale = nn.Parameter(torch.ones(1) * (1.0 / math.sqrt(self.head_dim)))\n",
        "\n",
        "        # Threshold parameters for attention scores\n",
        "        self.register_buffer('score_running_mean', torch.zeros(n_heads))\n",
        "        self.register_buffer('score_running_var', torch.ones(n_heads))\n",
        "        self.score_threshold = nn.Parameter(torch.ones(1) * 0.5)\n",
        "        self.score_momentum = 0.01\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Project to queries, keys, values\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "\n",
        "        # Compute scaled attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.attn_scale  # B, H, T, T\n",
        "\n",
        "        # Apply causal mask if provided\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask, float('-inf'))\n",
        "\n",
        "        # Apply thresholding to attention scores\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                # Compute statistics of attention scores across batch and tokens\n",
        "                # We remove the masked (very negative) values from statistics calculation\n",
        "                valid_mask = ~torch.isinf(scores)\n",
        "                if valid_mask.any():\n",
        "                    # Get head-wise mean and variance\n",
        "                    score_mean = torch.sum(scores * valid_mask, dim=(0, 2, 3)) / torch.sum(valid_mask, dim=(0, 2, 3))\n",
        "                    score_var = torch.sum(((scores - score_mean.view(1, -1, 1, 1)) ** 2) * valid_mask, dim=(0, 2, 3)) / torch.sum(valid_mask, dim=(0, 2, 3))\n",
        "\n",
        "                    # Update running statistics\n",
        "                    self.score_running_mean = (1 - self.score_momentum) * self.score_running_mean + self.score_momentum * score_mean\n",
        "                    self.score_running_var = (1 - self.score_momentum) * self.score_running_var + self.score_momentum * score_var\n",
        "\n",
        "        # Calculate adaptive threshold for attention scores\n",
        "        threshold_value = torch.sigmoid(self.score_threshold) * torch.sqrt(torch.clamp(self.score_running_var, min=1e-6))\n",
        "\n",
        "        # Create soft mask for scores (0 for values below threshold, 1 for values above)\n",
        "        # We can't use scores directly as they may have -inf values, so we'll make a mask\n",
        "        # Exclude values that are already -inf (from causal mask)\n",
        "        mask = (~torch.isinf(scores)) & (scores < threshold_value.view(1, -1, 1, 1))\n",
        "        scores = scores.masked_fill(mask, -1e4)  # Not -inf to keep gradients\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, v)  # B, H, T, D\n",
        "\n",
        "        # Reshape and project output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "    # Method to handle compatibility with original MultiheadAttention\n",
        "    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n",
        "        # Map old MHA parameters to new ThresholdedAttention parameters\n",
        "        if f\"{prefix}in_proj_weight\" in state_dict:\n",
        "            # MultiheadAttention uses a single in_proj_weight that combines q,k,v\n",
        "            in_proj_weight = state_dict.pop(f\"{prefix}in_proj_weight\")\n",
        "            in_proj_bias = state_dict.pop(f\"{prefix}in_proj_bias\", None)\n",
        "\n",
        "            # Split the in_proj_weight into q, k, v parts\n",
        "            q_weight, k_weight, v_weight = in_proj_weight.chunk(3, dim=0)\n",
        "            state_dict[f\"{prefix}q_proj.weight\"] = q_weight\n",
        "            state_dict[f\"{prefix}k_proj.weight\"] = k_weight\n",
        "            state_dict[f\"{prefix}v_proj.weight\"] = v_weight\n",
        "\n",
        "            if in_proj_bias is not None:\n",
        "                q_bias, k_bias, v_bias = in_proj_bias.chunk(3, dim=0)\n",
        "                state_dict[f\"{prefix}q_proj.bias\"] = q_bias\n",
        "                state_dict[f\"{prefix}k_proj.bias\"] = k_bias\n",
        "                state_dict[f\"{prefix}v_proj.bias\"] = v_bias\n",
        "\n",
        "        # Map out_proj parameters\n",
        "        if f\"{prefix}out_proj.weight\" in state_dict:\n",
        "            state_dict[f\"{prefix}out_proj.weight\"] = state_dict[f\"{prefix}out_proj.weight\"]\n",
        "            if f\"{prefix}out_proj.bias\" in state_dict:\n",
        "                state_dict[f\"{prefix}out_proj.bias\"] = state_dict[f\"{prefix}out_proj.bias\"]\n",
        "\n",
        "        # Call parent class method to handle the rest\n",
        "        super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n",
        "\n",
        "# ==========================================\n",
        "# 4) Improved Transformer Block with Thresholded Attention\n",
        "# ==========================================\n",
        "class ImprovedTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.attention = ThresholdedAttention(embed_dim, n_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            ImprovedEmergentThresholdLayer(4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        self.threshold1 = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "        self.threshold2 = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, E = x.size()\n",
        "        causal_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        attn_out = self.attention(x, attn_mask=causal_mask)\n",
        "        x = x + self.threshold1(attn_out)\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = x + self.threshold2(ff_out)\n",
        "        return x\n",
        "\n",
        "# ==========================================\n",
        "# 5) Improved Byte Transformer\n",
        "# ==========================================\n",
        "class ImprovedByteTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, n_heads=4, n_layers=4, block_size=128):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(self.block_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ImprovedTransformerBlock(embed_dim, n_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.final_threshold = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "        self.ln_f = nn.Linear(embed_dim, vocab_size)\n",
        "        # Learned gating parameter for combining memory outputs\n",
        "        self.gate_param = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward_with_embeddings(self, x_emb):\n",
        "        for block in self.blocks:\n",
        "            x_emb = block(x_emb)\n",
        "        x_emb = self.final_threshold(x_emb)\n",
        "        logits = self.ln_f(x_emb)\n",
        "        return logits\n",
        "\n",
        "    def forward_with_two_memory(self, x_emb, memory_module2):\n",
        "        \"\"\"\n",
        "        Extended forward pass:\n",
        "          1. Run transformer blocks on x_emb.\n",
        "          2. Apply the transformer's final threshold.\n",
        "          3. Process the result with a second memory module.\n",
        "          4. Combine the result of memory_module2 and the original x_emb using a gated combination.\n",
        "          5. Apply the final threshold on the combined representation.\n",
        "          6. Project to logits.\n",
        "        \"\"\"\n",
        "        transformer_out = x_emb\n",
        "        for block in self.blocks:\n",
        "            transformer_out = block(transformer_out)\n",
        "        transformer_out = self.final_threshold(transformer_out)\n",
        "        mem_out2 = memory_module2(transformer_out)\n",
        "        # Gated combination instead of simple addition:\n",
        "        alpha = torch.sigmoid(self.gate_param)  # Learned gating weight in [0, 1]\n",
        "        combined = alpha * mem_out2 + (1 - alpha) * x_emb\n",
        "        final_emb = self.final_threshold(combined)\n",
        "        logits = self.ln_f(final_emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.size()\n",
        "        token_emb = self.token_embedding(x)\n",
        "        positions = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "        pos_emb = self.pos_embedding(positions)\n",
        "        x_emb = token_emb + pos_emb\n",
        "        return self.forward_with_embeddings(x_emb)\n",
        "\n",
        "# ==========================================\n",
        "# 6) Memory Module (Original)\n",
        "# ==========================================\n",
        "class MemoryModule(nn.Module):\n",
        "    def __init__(self, embed_dim, n_layers=8, expansion_factor=4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            layer = nn.Sequential(\n",
        "                nn.LayerNorm(embed_dim),\n",
        "                nn.Linear(embed_dim, embed_dim * expansion_factor),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(embed_dim * expansion_factor, embed_dim),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "        self.final_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            out = out + layer(out)\n",
        "        out = self.final_norm(out)\n",
        "        return out\n",
        "\n",
        "# ==========================================\n",
        "# 7) Weighted Cross-Entropy Loss with KL Divergence Constraint\n",
        "# ==========================================\n",
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, token_frequencies, alpha=0.5, kl_lambda=0.1):\n",
        "        super().__init__()\n",
        "        self.register_buffer('weights', compute_weights_from_frequencies(token_frequencies, alpha))\n",
        "        self.kl_lambda = kl_lambda\n",
        "\n",
        "    def forward(self, logits, targets, mask=None):\n",
        "        \"\"\"\n",
        "        Compute weighted cross-entropy loss with KL divergence regularization\n",
        "\n",
        "        Args:\n",
        "            logits: Model output logits of shape [B*T, C]\n",
        "            targets: Target indices of shape [B*T]\n",
        "            mask: Optional mask for padding of shape [B*T]\n",
        "\n",
        "        Returns:\n",
        "            loss: Total loss combining weighted CE and KL divergence\n",
        "        \"\"\"\n",
        "        # Get class probabilities from logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Compute standard (unweighted) CE loss\n",
        "        standard_ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "\n",
        "        # Compute weighted CE loss\n",
        "        # We need to handle the weights for the specific target classes\n",
        "        B = targets.size(0)\n",
        "        weights_per_sample = self.weights[targets]\n",
        "        weighted_ce = standard_ce * weights_per_sample\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            standard_ce = standard_ce * mask\n",
        "            weighted_ce = weighted_ce * mask\n",
        "\n",
        "            # Normalize by sum of mask\n",
        "            mask_sum = mask.sum() + 1e-9\n",
        "            standard_ce = standard_ce.sum() / mask_sum\n",
        "            weighted_ce = weighted_ce.sum() / mask_sum\n",
        "        else:\n",
        "            standard_ce = standard_ce.mean()\n",
        "            weighted_ce = weighted_ce.mean()\n",
        "\n",
        "        # Compute unweighted model distribution\n",
        "        with torch.no_grad():\n",
        "            logits_detached = logits.detach()\n",
        "            unweighted_probs = F.softmax(logits_detached, dim=-1)\n",
        "\n",
        "        # For simplicity, we'll use a proxy for KL divergence regulation:\n",
        "        # We use the difference between weighted and unweighted loss as a regularizer\n",
        "        # This approximates the effect of limiting KL divergence between the two distributions\n",
        "        ce_diff = torch.abs(weighted_ce - standard_ce)\n",
        "\n",
        "        # Total loss with KL divergence proxy as regularization\n",
        "        total_loss = weighted_ce + self.kl_lambda * ce_diff\n",
        "\n",
        "        return total_loss, weighted_ce, ce_diff\n",
        "\n",
        "# ==========================================\n",
        "# 7.1) Pre-training Evaluation Function\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def estimate_loss_pretrain(main_model, memory1, memory2, train_bytes, val_bytes, weighted_loss_fn=None):\n",
        "    \"\"\"\n",
        "    Estimate loss on training and validation byte data.\n",
        "    This version works directly with byte tensors.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    main_model.eval()\n",
        "    memory1.eval()\n",
        "    memory2.eval()\n",
        "\n",
        "    for split, byte_tensor in [('train', train_bytes), ('val', val_bytes)]:\n",
        "        losses = torch.zeros(hyperparams['eval_iters'])\n",
        "\n",
        "        for k in range(hyperparams['eval_iters']):\n",
        "            # Get batches directly from byte tensors\n",
        "            try:\n",
        "                inputs, targets = prepare_byte_batches(\n",
        "                    byte_tensor,\n",
        "                    hyperparams['block_size'],\n",
        "                    hyperparams['batch_size'],\n",
        "                    device\n",
        "                )\n",
        "\n",
        "                # Forward pass\n",
        "                B, T = inputs.shape\n",
        "                token_emb = main_model.token_embedding(inputs)\n",
        "                pos_emb = main_model.pos_embedding(torch.arange(T, device=device).unsqueeze(0))\n",
        "                combined_emb = token_emb + pos_emb\n",
        "\n",
        "                mem_out1 = memory1(combined_emb)\n",
        "                logits = main_model.forward_with_two_memory(mem_out1, memory2)\n",
        "\n",
        "                # Calculate loss (only on non-padding tokens)\n",
        "                B, T, C = logits.shape\n",
        "                logits_flat = logits.view(B * T, C)\n",
        "                targets_flat = targets.view(B * T)\n",
        "\n",
        "                # Create mask for non-padding tokens\n",
        "                mask = (targets_flat != 0).float()\n",
        "\n",
        "                # Use weighted loss if provided, otherwise standard CE\n",
        "                if weighted_loss_fn is not None and hyperparams['use_weighted_loss']:\n",
        "                    loss, _, _ = weighted_loss_fn(logits_flat, targets_flat, mask)\n",
        "                else:\n",
        "                    # Compute loss only on non-padding tokens with standard CE\n",
        "                    loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
        "                    loss = (loss * mask).sum() / (mask.sum() + 1e-9)\n",
        "\n",
        "                losses[k] = loss.item()\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation: {e}\")\n",
        "                losses[k] = float('inf')  # Use a large value to indicate error\n",
        "\n",
        "        # Use median instead of mean to be more robust to outliers/errors\n",
        "        valid_losses = losses[losses != float('inf')]\n",
        "        if len(valid_losses) > 0:\n",
        "            out[split] = valid_losses.median().item()\n",
        "        else:\n",
        "            out[split] = float('inf')\n",
        "\n",
        "    main_model.train()\n",
        "    memory1.train()\n",
        "    memory2.train()\n",
        "    return out\n",
        "\n",
        "# ==========================================\n",
        "# 8) Generate Text from Trained Model\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def generate_from_prompt(main_model, memory1, memory2, prompt_text=None, max_new_tokens=200, top_p=None):\n",
        "    if prompt_text is None:\n",
        "        prompt_text = hyperparams['start_prompt']\n",
        "\n",
        "    # Use hyperparameter value if top_p not specified\n",
        "    if top_p is None:\n",
        "        top_p = hyperparams['top_p']\n",
        "\n",
        "    # Apply system prompt to user prompt\n",
        "    system_prompt = hyperparams['system_prompt']\n",
        "    full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt_text}\"\n",
        "\n",
        "    # Convert prompt to bytes\n",
        "    if isinstance(full_prompt, str):\n",
        "        prompt_bytes = full_prompt.encode('utf-8')\n",
        "    elif not isinstance(full_prompt, bytes):\n",
        "        prompt_bytes = str(full_prompt).encode('utf-8')\n",
        "\n",
        "    main_model.eval()\n",
        "    memory1.eval()\n",
        "    memory2.eval()\n",
        "\n",
        "    # Create context from prompt\n",
        "    context = torch.tensor([b for b in prompt_bytes], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Add BOS token to start the response generation\n",
        "    bos_token = torch.tensor([[hyperparams['bos_token']]], dtype=torch.long, device=device)\n",
        "    context = torch.cat([context, bos_token], dim=1)\n",
        "\n",
        "    generated = []\n",
        "    eos_found = False\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_found:\n",
        "            break\n",
        "\n",
        "        x_cond = context[:, -hyperparams['block_size']:] if context.size(1) > hyperparams['block_size'] else context\n",
        "        B, T = x_cond.shape\n",
        "        token_emb = main_model.token_embedding(x_cond)\n",
        "        pos_emb = main_model.pos_embedding(torch.arange(T, device=x_cond.device).unsqueeze(0))\n",
        "        combined_emb = token_emb + pos_emb\n",
        "\n",
        "        mem_out1 = memory1(combined_emb)\n",
        "        logits = main_model.forward_with_two_memory(mem_out1, memory2)\n",
        "\n",
        "        # Get next token distribution with top-p (nucleus) sampling\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sort probabilities in descending order\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "        # Compute cumulative probabilities\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Find indices where cumulative probability exceeds top_p\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        # Shift to create first index (0) as False to always keep at least one token\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Create mask for indices to remove\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "\n",
        "        # Filter logits\n",
        "        filtered_logits = logits.clone()\n",
        "        filtered_logits[indices_to_remove] = -float('inf')\n",
        "\n",
        "        # Get probabilities from filtered logits\n",
        "        filtered_probs = F.softmax(filtered_logits, dim=-1)\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        next_token = torch.multinomial(filtered_probs, num_samples=1)\n",
        "        next_token_value = next_token.item()\n",
        "\n",
        "        # Check for EOS token\n",
        "        if next_token_value == hyperparams['eos_token']:\n",
        "            eos_found = True\n",
        "\n",
        "        generated.append(next_token_value)\n",
        "        context = torch.cat([context, next_token], dim=1)\n",
        "\n",
        "    # Combine context with generated bytes and return as bytes object\n",
        "    result_bytes = bytes(context.view(-1).tolist())\n",
        "\n",
        "    # Clean up special tokens when returning result\n",
        "    try:\n",
        "        # Convert to list for easier manipulation\n",
        "        byte_list = list(result_bytes)\n",
        "\n",
        "        # Find all BOS tokens and remove them\n",
        "        while hyperparams['bos_token'] in byte_list:\n",
        "            byte_list.remove(hyperparams['bos_token'])\n",
        "\n",
        "        # Find all EOS tokens and remove everything after the first one\n",
        "        if hyperparams['eos_token'] in byte_list:\n",
        "            eos_index = byte_list.index(hyperparams['eos_token'])\n",
        "            byte_list = byte_list[:eos_index]\n",
        "\n",
        "        # Convert back to bytes\n",
        "        cleaned_bytes = bytes(byte_list)\n",
        "        return cleaned_bytes\n",
        "    except:\n",
        "        # If any error in cleaning, return the original bytes\n",
        "        return result_bytes\n",
        "\n",
        "# ==========================================\n",
        "# 9) Pre-training Implementation\n",
        "# ==========================================\n",
        "def pretrain(continue_training=True):\n",
        "    \"\"\"Pre-train the model on Omega Books corpus with causal language modeling.\"\"\"\n",
        "    # Load Omega Books data as bytes\n",
        "    _, _, train_bytes_tensor, val_bytes_tensor = load_omega_books_data_as_bytes()\n",
        "\n",
        "    # Create models\n",
        "    main_model = ImprovedByteTransformer(\n",
        "        vocab_size=hyperparams['vocab_size'],\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_heads=hyperparams['n_heads'],\n",
        "        n_layers=hyperparams['n_layers'],\n",
        "        block_size=hyperparams['block_size']\n",
        "    ).to(device)\n",
        "\n",
        "    memory1 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    memory2 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    # Calculate model size\n",
        "    num_params = sum(p.numel() for p in main_model.parameters() if p.requires_grad)\n",
        "    num_params += sum(p.numel() for p in memory1.parameters() if p.requires_grad)\n",
        "    num_params += sum(p.numel() for p in memory2.parameters() if p.requires_grad)\n",
        "    print(f\"Total trainable parameters: {num_params:,}\")\n",
        "\n",
        "    # Compute token frequencies and initialize weighted loss if enabled\n",
        "    weighted_loss_fn = None\n",
        "    if hyperparams['use_weighted_loss']:\n",
        "        # Compute token frequencies from training data\n",
        "        token_frequencies = compute_token_frequencies(\n",
        "            train_bytes_tensor,\n",
        "            vocab_size=hyperparams['vocab_size']\n",
        "        )\n",
        "\n",
        "        # Initialize weighted loss\n",
        "        weighted_loss_fn = WeightedCrossEntropyLoss(\n",
        "            token_frequencies=token_frequencies,\n",
        "            alpha=hyperparams['alpha'],\n",
        "            kl_lambda=hyperparams['kl_lambda']\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"Using weighted cross-entropy loss with alpha={hyperparams['alpha']}, kl_lambda={hyperparams['kl_lambda']}\")\n",
        "    else:\n",
        "        print(\"Using standard cross-entropy loss\")\n",
        "\n",
        "    # Optimizer setup\n",
        "    group1_params = list(main_model.parameters()) + list(memory1.parameters())\n",
        "    group2_params = list(memory2.parameters())\n",
        "    base_lr = 3e-4\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': group1_params, 'lr': base_lr},\n",
        "        {'params': group2_params, 'lr': base_lr}\n",
        "    ], betas=(0.9, 0.95), weight_decay=0.1)\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Load checkpoint if continuing training\n",
        "    if continue_training and os.path.exists(hyperparams['checkpoint_path']):\n",
        "        try:\n",
        "            print(f\"Loading checkpoint from {hyperparams['checkpoint_path']}...\")\n",
        "            checkpoint = torch.load(hyperparams['checkpoint_path'], map_location=device)\n",
        "\n",
        "            try:\n",
        "                # Try to load model states directly\n",
        "                main_model.load_state_dict(checkpoint['main_model_state'], strict=False)\n",
        "                memory1.load_state_dict(checkpoint['memory1_state'])\n",
        "                if 'memory2_state' in checkpoint:\n",
        "                    memory2.load_state_dict(checkpoint['memory2_state'])\n",
        "                if 'optimizer_state' in checkpoint:\n",
        "                    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "                start_epoch = checkpoint.get('epoch', 0)\n",
        "                best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "                print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading checkpoint directly: {e}\")\n",
        "                print(\"Starting pre-training from scratch.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load checkpoint: {e}\")\n",
        "            print(\"Starting pre-training from scratch.\")\n",
        "    else:\n",
        "        print(\"Starting pre-training from scratch.\")\n",
        "\n",
        "    # Training setup\n",
        "    grad_clip = 1.0\n",
        "    total_steps = hyperparams['num_epochs'] * hyperparams['steps_per_epoch']\n",
        "    current_step = start_epoch * hyperparams['steps_per_epoch']\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def get_lr(step, warmup_steps=2000, base_lr=base_lr, min_lr=1e-5):\n",
        "        # Learning rate schedule with warmup and cosine decay\n",
        "        if step < warmup_steps:\n",
        "            return base_lr * step / warmup_steps\n",
        "        decay_steps = total_steps - warmup_steps\n",
        "        step_ = step - warmup_steps\n",
        "        cosine_decay = 0.5 * (1 + math.cos(math.pi * step_ / decay_steps))\n",
        "        return min_lr + (base_lr - min_lr) * cosine_decay\n",
        "\n",
        "    print(\"Starting pre-training on Omega Books corpus...\")\n",
        "    for epoch in range(start_epoch, hyperparams['num_epochs']):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{hyperparams['num_epochs']} ---\")\n",
        "\n",
        "        for step in range(hyperparams['steps_per_epoch']):\n",
        "            # Periodic evaluation\n",
        "            if step % hyperparams['eval_interval'] == 0:\n",
        "                losses = estimate_loss_pretrain(main_model, memory1, memory2, train_bytes_tensor, val_bytes_tensor, weighted_loss_fn)\n",
        "                print(f\"Step {step}, train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "                # Save best model\n",
        "                if losses['val'] < best_val_loss:\n",
        "                    best_val_loss = losses['val']\n",
        "                    torch.save({\n",
        "                        'main_model_state': main_model.state_dict(),\n",
        "                        'memory1_state': memory1.state_dict(),\n",
        "                        'memory2_state': memory2.state_dict(),\n",
        "                        'optimizer_state': optimizer.state_dict(),\n",
        "                        'epoch': epoch,\n",
        "                        'val_loss': best_val_loss\n",
        "                    }, hyperparams['checkpoint_path'].replace('.pt', '_best.pt'))\n",
        "                    print(f\"New best model saved! Val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            # Get batches for this step directly from byte tensors\n",
        "            inputs, targets = prepare_byte_batches(\n",
        "                train_bytes_tensor,\n",
        "                hyperparams['block_size'],\n",
        "                hyperparams['batch_size'],\n",
        "                device\n",
        "            )\n",
        "\n",
        "            # Zero gradients\n",
        "            if step % hyperparams['accumulation_steps'] == 0:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            B, T = inputs.shape\n",
        "            token_emb = main_model.token_embedding(inputs)\n",
        "            pos_emb = main_model.pos_embedding(torch.arange(T, device=device).unsqueeze(0))\n",
        "            combined_emb = token_emb + pos_emb\n",
        "\n",
        "            mem_out1 = memory1(combined_emb)\n",
        "            logits = main_model.forward_with_two_memory(mem_out1, memory2)\n",
        "\n",
        "            # Calculate loss\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            mask = (targets_flat != 0).float()\n",
        "\n",
        "            # Use weighted loss if enabled\n",
        "            if weighted_loss_fn is not None and hyperparams['use_weighted_loss']:\n",
        "                loss, weighted_ce, ce_diff = weighted_loss_fn(logits_flat, targets_flat, mask)\n",
        "            else:\n",
        "                # Standard CE loss\n",
        "                loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
        "                loss = (loss * mask).sum() / (mask.sum() + 1e-9)\n",
        "\n",
        "            # Scale loss for gradient accumulation\n",
        "            scaled_loss = loss / hyperparams['accumulation_steps']\n",
        "            scaled_loss.backward()\n",
        "\n",
        "            # Check for NaN or Inf gradients\n",
        "            has_nan_inf = False\n",
        "            for param in main_model.parameters():\n",
        "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
        "                    has_nan_inf = True\n",
        "                    param.grad = torch.zeros_like(param.grad)\n",
        "\n",
        "            if has_nan_inf:\n",
        "                print(f\"NaN or Inf gradients detected and zeroed at step {step}\")\n",
        "\n",
        "            # Apply optimizer step\n",
        "            if (step + 1) % hyperparams['accumulation_steps'] == 0:\n",
        "                # Update learning rate\n",
        "                lr = get_lr(current_step)\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "\n",
        "                # Clip gradients\n",
        "                torch.nn.utils.clip_grad_norm_(main_model.parameters(), grad_clip)\n",
        "                torch.nn.utils.clip_grad_norm_(memory1.parameters(), grad_clip)\n",
        "                torch.nn.utils.clip_grad_norm_(memory2.parameters(), grad_clip)\n",
        "\n",
        "                optimizer.step()\n",
        "                current_step += 1\n",
        "\n",
        "        # Generate sample at end of epoch\n",
        "        try:\n",
        "            print(\"\\nGenerating sample text...\")\n",
        "            sample_text = generate_from_prompt(\n",
        "                main_model, memory1, memory2,\n",
        "                prompt_text=hyperparams['start_prompt'],\n",
        "                max_new_tokens=256\n",
        "            )\n",
        "            # Try to decode the bytes to show readable text\n",
        "            try:\n",
        "                decoded_text = sample_text.decode('utf-8', errors='replace')\n",
        "                print(f\"Sample: {decoded_text[:500]}\")\n",
        "            except:\n",
        "                print(f\"Sample (raw bytes, could not decode): {sample_text[:200]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating sample: {e}\")\n",
        "\n",
        "        # End of epoch checkpoint\n",
        "        torch.save({\n",
        "            'main_model_state': main_model.state_dict(),\n",
        "            'memory1_state': memory1.state_dict(),\n",
        "            'memory2_state': memory2.state_dict(),\n",
        "            'optimizer_state': optimizer.state_dict(),\n",
        "            'epoch': epoch + 1,\n",
        "            'val_loss': best_val_loss\n",
        "        }, hyperparams['checkpoint_path'])\n",
        "        print(f\"Checkpoint saved at epoch {epoch+1} to {hyperparams['checkpoint_path']}.\")\n",
        "\n",
        "    print(\"Pre-training complete!\")\n",
        "\n",
        "# ==========================================\n",
        "# 10) Script Main Entry Point\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Starting pre-training on Omega Books corpus...\")\n",
        "    pretrain(continue_training=hyperparams['continue_training'])"
      ],
      "metadata": {
        "id": "UIfjfYQZDDIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pretraining V2.2 (sft dataset) fixed ce loss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==========================================\n",
        "# 1) Hyperparameters (same as original with additions)\n",
        "# ==========================================\n",
        "hyperparams = {\n",
        "    # Model Architecture\n",
        "    'block_size': 1024,               # Sequence length for context\n",
        "    'batch_size': 2,                  # Batch size\n",
        "    'embed_dim': 1024,                # Transformer embedding dimension\n",
        "    'n_heads': 16,                    # Number of attention heads\n",
        "    'n_layers': 24,                   # Number of Transformer blocks\n",
        "    'memory_n_layers': 8,             # Number of layers in the original MemoryModule\n",
        "    'vocab_size': 256,                # Fixed vocabulary size for byte tokenization\n",
        "\n",
        "    # Training Parameters\n",
        "    'num_epochs': 120,                 # Number of epochs\n",
        "    'steps_per_epoch': 1000,          # Steps per epoch\n",
        "    'eval_interval': 200,             # Steps between loss evaluations\n",
        "    'eval_iters': 100,                # Iterations to average validation loss\n",
        "    'accumulation_steps': 8,          # Number of steps to accumulate gradients over\n",
        "\n",
        "    # Generation Parameters\n",
        "    'generate_num_tokens': 2048,      # Number of tokens to generate after each epoch\n",
        "    'top_p': 0.8,                     # Top-p (nucleus) sampling parameter\n",
        "    'start_prompt': \"Explain why the statement 'I wore my lucky socks today, and I got an A on my test, so my socks must be lucky' is a logical fallacy.\",\n",
        "\n",
        "    # Special Tokens & Tags\n",
        "    'thinking_tag': \"<think>\",        # Opening tag for thinking process\n",
        "    'thinking_end_tag': \"</think>\",   # Closing tag for thinking process\n",
        "    'answer_tag': \"<answer>\",         # Opening tag for final answer\n",
        "    'answer_end_tag': \"</answer>\",    # Closing tag for final answer\n",
        "    'bos_token': 254,                 # Beginning-of-sequence token (byte value)\n",
        "    'eos_token': 255,                 # End-of-sequence token (byte value)\n",
        "\n",
        "    # File Paths & Modes\n",
        "    'checkpoint_path': \"threshold_transformer_checkpoint.pt\",  # Single unified checkpoint\n",
        "    'mode': 'pretrain',               # Force pretrain mode\n",
        "    'continue_training': True,        # Whether to continue training from a checkpoint\n",
        "    'system_prompt': \"\"\"just think before answer.\"\"\"\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 1.1) Select device\n",
        "# ==========================================\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \\\n",
        "         (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==========================================\n",
        "# 1.2) Data Loading and Preprocessing for COT Logic Reasoning\n",
        "# ==========================================\n",
        "def load_cot_logic_data():\n",
        "    print(\"Loading COT Logic Reasoning dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Try standard pandas read_parquet first\n",
        "        df = pd.read_parquet(\"isaiahbjork/cot-logic-reasoning/cot-logic-reasoning.parquet\")\n",
        "        print(\"Dataset loaded using standard path\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset with standard path: {e}\")\n",
        "        try:\n",
        "            # Try with datasets library if available\n",
        "            try:\n",
        "                from datasets import load_dataset\n",
        "                dataset = load_dataset(\"isaiahbjork/cot-logic-reasoning\")\n",
        "                df = dataset[\"train\"].to_pandas()\n",
        "                print(\"Dataset loaded using datasets library\")\n",
        "            except:\n",
        "                # If all else fails, use the original path format\n",
        "                df = pd.read_parquet(\"hf://datasets/isaiahbjork/cot-logic-reasoning/cot-logic-reasoning.parquet\")\n",
        "                print(\"Dataset loaded using hf:// protocol\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Failed to load dataset: {e2}\")\n",
        "            raise RuntimeError(\"Unable to load the COT Logic Reasoning dataset\")\n",
        "\n",
        "    print(f\"Data size: {len(df)}\")\n",
        "\n",
        "    # Split into train/validation/test sets (80/10/10)\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    print(f\"Training examples: {len(train_df)}\")\n",
        "    print(f\"Validation examples: {len(val_df)}\")\n",
        "    print(f\"Test examples: {len(test_df)}\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# ==========================================\n",
        "# 1.3) Prepare data for pre-training (continuous text with BOS/EOS tokens)\n",
        "# ==========================================\n",
        "def prepare_pretraining_batches_from_cot(data_df, block_size=1024):\n",
        "    \"\"\"Create pre-training batches from COT corpus as continuous text for next-token prediction,\n",
        "    but with BOS/EOS tokens around answers.\"\"\"\n",
        "\n",
        "    batch_indices = torch.randint(0, len(data_df), (hyperparams['batch_size'],))\n",
        "    batch_examples = data_df.iloc[batch_indices]\n",
        "\n",
        "    sequences = []\n",
        "\n",
        "    for _, row in batch_examples.iterrows():\n",
        "        # Get prompt and response\n",
        "        prompt = row['prompt']\n",
        "        response = row['response']\n",
        "\n",
        "        # Add system prompt to the user prompt\n",
        "        system_prompt = hyperparams['system_prompt']\n",
        "        full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt}\"\n",
        "\n",
        "        # Insert BOS before response and EOS after response\n",
        "        # But no formatting tags - treat as continuous text\n",
        "        full_text = full_prompt + chr(hyperparams['bos_token']) + response + chr(hyperparams['eos_token'])\n",
        "\n",
        "        # Convert to byte sequence\n",
        "        byte_seq = [b for b in full_text.encode('utf-8')]\n",
        "\n",
        "        # Truncate or pad to block_size\n",
        "        if len(byte_seq) > block_size:\n",
        "            # Random offset for diverse training\n",
        "            start_idx = torch.randint(0, len(byte_seq) - block_size, (1,)).item()\n",
        "            byte_seq = byte_seq[start_idx:start_idx + block_size]\n",
        "        else:\n",
        "            byte_seq = byte_seq + [0] * (block_size - len(byte_seq))\n",
        "\n",
        "        sequences.append(byte_seq)\n",
        "\n",
        "    # Convert to tensor\n",
        "    x = torch.tensor(sequences, dtype=torch.long).to(device)\n",
        "\n",
        "    # Create targets by shifting input by 1 position\n",
        "    y = torch.full_like(x, 0)\n",
        "    y[:, :-1] = x[:, 1:].clone()\n",
        "    y[:, -1] = 0  # Last position predicts padding\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# ==========================================\n",
        "# 2) Improved Emergent Threshold Layer with Numerical Stability\n",
        "# ==========================================\n",
        "class ImprovedEmergentThresholdLayer(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.norm = nn.LayerNorm(feature_dim)\n",
        "        self.register_buffer('running_mean', torch.zeros(feature_dim))\n",
        "        self.register_buffer('running_var', torch.ones(feature_dim))\n",
        "        self.adaptive_threshold = nn.Parameter(torch.ones(1) * 0.5)\n",
        "        self.momentum = 0.01\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = self.norm(x)\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                batch_mean = x_norm.mean(dim=(0, 1))\n",
        "                batch_var = x_norm.var(dim=(0, 1), unbiased=False)\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "\n",
        "        # More robust threshold calculation with clamping to prevent extremely small values\n",
        "        threshold = torch.sigmoid(self.adaptive_threshold) * torch.sqrt(torch.clamp(self.running_var, min=1e-6))\n",
        "\n",
        "        # Increase denominator from 0.1 to 1.0 for stability\n",
        "        gate = torch.sigmoid((torch.abs(x_norm) - threshold.view(1, 1, -1)) / 1.0)\n",
        "\n",
        "        alpha = torch.sigmoid(self.adaptive_threshold)\n",
        "\n",
        "        # Clip outputs to prevent extreme values\n",
        "        return torch.clamp(alpha * (gate * x) + (1 - alpha) * x, min=-100, max=100)\n",
        "\n",
        "# ==========================================\n",
        "# 3) Thresholded Attention Mechanism\n",
        "# ==========================================\n",
        "class ThresholdedAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        # Standard attention projections\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Attention score normalization\n",
        "        self.attn_scale = nn.Parameter(torch.ones(1) * (1.0 / math.sqrt(self.head_dim)))\n",
        "\n",
        "        # Threshold parameters for attention scores\n",
        "        self.register_buffer('score_running_mean', torch.zeros(n_heads))\n",
        "        self.register_buffer('score_running_var', torch.ones(n_heads))\n",
        "        self.score_threshold = nn.Parameter(torch.ones(1) * 0.5)\n",
        "        self.score_momentum = 0.01\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Project to queries, keys, values\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T, D\n",
        "\n",
        "        # Compute scaled attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.attn_scale  # B, H, T, T\n",
        "\n",
        "        # Apply causal mask if provided\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask, float('-inf'))\n",
        "\n",
        "        # Apply thresholding to attention scores\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                # Compute statistics of attention scores across batch and tokens\n",
        "                # We remove the masked (very negative) values from statistics calculation\n",
        "                valid_mask = ~torch.isinf(scores)\n",
        "                if valid_mask.any():\n",
        "                    # Get head-wise mean and variance\n",
        "                    score_mean = torch.sum(scores * valid_mask, dim=(0, 2, 3)) / torch.sum(valid_mask, dim=(0, 2, 3))\n",
        "                    score_var = torch.sum(((scores - score_mean.view(1, -1, 1, 1)) ** 2) * valid_mask, dim=(0, 2, 3)) / torch.sum(valid_mask, dim=(0, 2, 3))\n",
        "\n",
        "                    # Update running statistics\n",
        "                    self.score_running_mean = (1 - self.score_momentum) * self.score_running_mean + self.score_momentum * score_mean\n",
        "                    self.score_running_var = (1 - self.score_momentum) * self.score_running_var + self.score_momentum * score_var\n",
        "\n",
        "        # Calculate adaptive threshold for attention scores\n",
        "        threshold_value = torch.sigmoid(self.score_threshold) * torch.sqrt(torch.clamp(self.score_running_var, min=1e-6))\n",
        "\n",
        "        # Create soft mask for scores (0 for values below threshold, 1 for values above)\n",
        "        # We can't use scores directly as they may have -inf values, so we'll make a mask\n",
        "        # Exclude values that are already -inf (from causal mask)\n",
        "        mask = (~torch.isinf(scores)) & (scores < threshold_value.view(1, -1, 1, 1))\n",
        "        scores = scores.masked_fill(mask, -1e4)  # Not -inf to keep gradients\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, v)  # B, H, T, D\n",
        "\n",
        "        # Reshape and project output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "    # Method to handle compatibility with original MultiheadAttention\n",
        "    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n",
        "        # Map old MHA parameters to new ThresholdedAttention parameters\n",
        "        if f\"{prefix}in_proj_weight\" in state_dict:\n",
        "            # MultiheadAttention uses a single in_proj_weight that combines q,k,v\n",
        "            in_proj_weight = state_dict.pop(f\"{prefix}in_proj_weight\")\n",
        "            in_proj_bias = state_dict.pop(f\"{prefix}in_proj_bias\", None)\n",
        "\n",
        "            # Split the in_proj_weight into q, k, v parts\n",
        "            q_weight, k_weight, v_weight = in_proj_weight.chunk(3, dim=0)\n",
        "            state_dict[f\"{prefix}q_proj.weight\"] = q_weight\n",
        "            state_dict[f\"{prefix}k_proj.weight\"] = k_weight\n",
        "            state_dict[f\"{prefix}v_proj.weight\"] = v_weight\n",
        "\n",
        "            if in_proj_bias is not None:\n",
        "                q_bias, k_bias, v_bias = in_proj_bias.chunk(3, dim=0)\n",
        "                state_dict[f\"{prefix}q_proj.bias\"] = q_bias\n",
        "                state_dict[f\"{prefix}k_proj.bias\"] = k_bias\n",
        "                state_dict[f\"{prefix}v_proj.bias\"] = v_bias\n",
        "\n",
        "        # Map out_proj parameters\n",
        "        if f\"{prefix}out_proj.weight\" in state_dict:\n",
        "            state_dict[f\"{prefix}out_proj.weight\"] = state_dict[f\"{prefix}out_proj.weight\"]\n",
        "            if f\"{prefix}out_proj.bias\" in state_dict:\n",
        "                state_dict[f\"{prefix}out_proj.bias\"] = state_dict[f\"{prefix}out_proj.bias\"]\n",
        "\n",
        "        # Call parent class method to handle the rest\n",
        "        super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n",
        "\n",
        "# ==========================================\n",
        "# 4) Improved Transformer Block with Thresholded Attention\n",
        "# ==========================================\n",
        "class ImprovedTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.attention = ThresholdedAttention(embed_dim, n_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            ImprovedEmergentThresholdLayer(4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        self.threshold1 = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "        self.threshold2 = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, E = x.size()\n",
        "        causal_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        attn_out = self.attention(x, attn_mask=causal_mask)\n",
        "        x = x + self.threshold1(attn_out)\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = x + self.threshold2(ff_out)\n",
        "        return x\n",
        "\n",
        "# ==========================================\n",
        "# 5) Improved Byte Transformer\n",
        "# ==========================================\n",
        "class ImprovedByteTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, n_heads=4, n_layers=4, block_size=128):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(self.block_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ImprovedTransformerBlock(embed_dim, n_heads)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.final_threshold = ImprovedEmergentThresholdLayer(embed_dim)\n",
        "        self.ln_f = nn.Linear(embed_dim, vocab_size)\n",
        "        # Learned gating parameter for combining memory outputs\n",
        "        self.gate_param = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward_with_embeddings(self, x_emb):\n",
        "        for block in self.blocks:\n",
        "            x_emb = block(x_emb)\n",
        "        x_emb = self.final_threshold(x_emb)\n",
        "        logits = self.ln_f(x_emb)\n",
        "        return logits\n",
        "\n",
        "    def forward_with_two_memory(self, x_emb, memory_module2):\n",
        "        \"\"\"\n",
        "        Extended forward pass:\n",
        "          1. Run transformer blocks on x_emb.\n",
        "          2. Apply the transformer's final threshold.\n",
        "          3. Process the result with a second memory module.\n",
        "          4. Combine the result of memory_module2 and the original x_emb using a gated combination.\n",
        "          5. Apply the final threshold on the combined representation.\n",
        "          6. Project to logits.\n",
        "        \"\"\"\n",
        "        transformer_out = x_emb\n",
        "        for block in self.blocks:\n",
        "            transformer_out = block(transformer_out)\n",
        "        transformer_out = self.final_threshold(transformer_out)\n",
        "        mem_out2 = memory_module2(transformer_out)\n",
        "        # Gated combination instead of simple addition:\n",
        "        alpha = torch.sigmoid(self.gate_param)  # Learned gating weight in [0, 1]\n",
        "        combined = alpha * mem_out2 + (1 - alpha) * x_emb\n",
        "        final_emb = self.final_threshold(combined)\n",
        "        logits = self.ln_f(final_emb)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.size()\n",
        "        token_emb = self.token_embedding(x)\n",
        "        positions = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "        pos_emb = self.pos_embedding(positions)\n",
        "        x_emb = token_emb + pos_emb\n",
        "        return self.forward_with_embeddings(x_emb)\n",
        "\n",
        "# ==========================================\n",
        "# 6) Memory Module (Original)\n",
        "# ==========================================\n",
        "class MemoryModule(nn.Module):\n",
        "    def __init__(self, embed_dim, n_layers=8, expansion_factor=4):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(n_layers):\n",
        "            layer = nn.Sequential(\n",
        "                nn.LayerNorm(embed_dim),\n",
        "                nn.Linear(embed_dim, embed_dim * expansion_factor),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(embed_dim * expansion_factor, embed_dim),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "        self.final_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            out = out + layer(out)\n",
        "        out = self.final_norm(out)\n",
        "        return out\n",
        "\n",
        "# ==========================================\n",
        "# 7) Pre-training Evaluation Function\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def estimate_loss_pretrain(main_model, memory1, memory2, train_df, val_df):\n",
        "    out = {}\n",
        "    main_model.eval()\n",
        "    memory1.eval()\n",
        "    memory2.eval()\n",
        "\n",
        "    for split, df in [('train', train_df), ('val', val_df)]:\n",
        "        losses = torch.zeros(hyperparams['eval_iters'])\n",
        "        for k in range(hyperparams['eval_iters']):\n",
        "            # Get pre-training batches\n",
        "            inputs, targets = prepare_pretraining_batches_from_cot(df, hyperparams['block_size'])\n",
        "\n",
        "            # Forward pass\n",
        "            B, T = inputs.shape\n",
        "            token_emb = main_model.token_embedding(inputs)\n",
        "            pos_emb = main_model.pos_embedding(torch.arange(T, device=device).unsqueeze(0))\n",
        "            combined_emb = token_emb + pos_emb\n",
        "\n",
        "            mem_out1 = memory1(combined_emb)\n",
        "            logits = main_model.forward_with_two_memory(mem_out1, memory2)\n",
        "\n",
        "            # Calculate loss (only on non-padding tokens)\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "\n",
        "            # Create mask for non-padding tokens\n",
        "            mask = (targets_flat != 0).float()\n",
        "\n",
        "            # Compute loss only on non-padding tokens\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
        "            masked_loss = (loss * mask).sum() / (mask.sum() + 1e-9)\n",
        "\n",
        "            losses[k] = masked_loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    main_model.train()\n",
        "    memory1.train()\n",
        "    memory2.train()\n",
        "    return out\n",
        "\n",
        "# ==========================================\n",
        "# 8) Generate Text from Trained Model\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def generate_from_prompt(main_model, memory1, memory2, prompt_text=None, max_new_tokens=200, top_p=None):\n",
        "    if prompt_text is None:\n",
        "        prompt_text = hyperparams['start_prompt']\n",
        "\n",
        "    # Use hyperparameter value if top_p not specified\n",
        "    if top_p is None:\n",
        "        top_p = hyperparams['top_p']\n",
        "\n",
        "    # Apply system prompt to user prompt\n",
        "    system_prompt = hyperparams['system_prompt']\n",
        "    full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt_text}\"\n",
        "\n",
        "    # Convert prompt to bytes\n",
        "    if isinstance(full_prompt, str):\n",
        "        prompt_bytes = full_prompt.encode('utf-8')\n",
        "    elif not isinstance(full_prompt, bytes):\n",
        "        prompt_bytes = str(full_prompt).encode('utf-8')\n",
        "\n",
        "    main_model.eval()\n",
        "    memory1.eval()\n",
        "    memory2.eval()\n",
        "\n",
        "    # Create context from prompt\n",
        "    context = torch.tensor([b for b in prompt_bytes], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Add BOS token to start the response generation\n",
        "    bos_token = torch.tensor([[hyperparams['bos_token']]], dtype=torch.long, device=device)\n",
        "    context = torch.cat([context, bos_token], dim=1)\n",
        "\n",
        "    generated = []\n",
        "    eos_found = False\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if eos_found:\n",
        "            break\n",
        "\n",
        "        x_cond = context[:, -hyperparams['block_size']:] if context.size(1) > hyperparams['block_size'] else context\n",
        "        B, T = x_cond.shape\n",
        "        token_emb = main_model.token_embedding(x_cond)\n",
        "        pos_emb = main_model.pos_embedding(torch.arange(T, device=x_cond.device).unsqueeze(0))\n",
        "        combined_emb = token_emb + pos_emb\n",
        "\n",
        "        mem_out1 = memory1(combined_emb)\n",
        "        logits = main_model.forward_with_two_memory(mem_out1, memory2)\n",
        "\n",
        "        # Get next token distribution with top-p (nucleus) sampling\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sort probabilities in descending order\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "        # Compute cumulative probabilities\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Find indices where cumulative probability exceeds top_p\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        # Shift to create first index (0) as False to always keep at least one token\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Create mask for indices to remove\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "\n",
        "        # Filter logits\n",
        "        filtered_logits = logits.clone()\n",
        "        filtered_logits[indices_to_remove] = -float('inf')\n",
        "\n",
        "        # Get probabilities from filtered logits\n",
        "        filtered_probs = F.softmax(filtered_logits, dim=-1)\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        next_token = torch.multinomial(filtered_probs, num_samples=1)\n",
        "        next_token_value = next_token.item()\n",
        "\n",
        "        # Check for EOS token\n",
        "        if next_token_value == hyperparams['eos_token']:\n",
        "            eos_found = True\n",
        "\n",
        "        generated.append(next_token_value)\n",
        "        context = torch.cat([context, next_token], dim=1)\n",
        "\n",
        "    # Combine context with generated bytes and return as bytes object\n",
        "    result_bytes = bytes(context.view(-1).tolist())\n",
        "\n",
        "    # Clean up special tokens when returning result\n",
        "    try:\n",
        "        # Convert to list for easier manipulation\n",
        "        byte_list = list(result_bytes)\n",
        "\n",
        "        # Find all BOS tokens and remove them\n",
        "        while hyperparams['bos_token'] in byte_list:\n",
        "            byte_list.remove(hyperparams['bos_token'])\n",
        "\n",
        "        # Find all EOS tokens and remove everything after the first one\n",
        "        if hyperparams['eos_token'] in byte_list:\n",
        "            eos_index = byte_list.index(hyperparams['eos_token'])\n",
        "            byte_list = byte_list[:eos_index]\n",
        "\n",
        "        # Convert back to bytes\n",
        "        cleaned_bytes = bytes(byte_list)\n",
        "        return cleaned_bytes\n",
        "    except:\n",
        "        # If any error in cleaning, return the original bytes\n",
        "        return result_bytes\n",
        "\n",
        "# ==========================================\n",
        "# 9) Pre-training Implementation\n",
        "# ==========================================\n",
        "def pretrain(continue_training=True):\n",
        "    \"\"\"Pre-train the model on COT corpus with causal language modeling.\"\"\"\n",
        "    # Load COT data\n",
        "    train_df, val_df, test_df = load_cot_logic_data()\n",
        "\n",
        "    # Create models\n",
        "    main_model = ImprovedByteTransformer(\n",
        "        vocab_size=hyperparams['vocab_size'],\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_heads=hyperparams['n_heads'],\n",
        "        n_layers=hyperparams['n_layers'],\n",
        "        block_size=hyperparams['block_size']\n",
        "    ).to(device)\n",
        "\n",
        "    memory1 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    memory2 = MemoryModule(\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_layers=hyperparams['memory_n_layers'],\n",
        "        expansion_factor=4\n",
        "    ).to(device)\n",
        "\n",
        "    # Calculate model size\n",
        "    num_params = sum(p.numel() for p in main_model.parameters() if p.requires_grad)\n",
        "    num_params += sum(p.numel() for p in memory1.parameters() if p.requires_grad)\n",
        "    num_params += sum(p.numel() for p in memory2.parameters() if p.requires_grad)\n",
        "    print(f\"Total trainable parameters: {num_params:,}\")\n",
        "\n",
        "    # Optimizer setup\n",
        "    group1_params = list(main_model.parameters()) + list(memory1.parameters())\n",
        "    group2_params = list(memory2.parameters())\n",
        "    base_lr = 3e-4\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': group1_params, 'lr': base_lr},\n",
        "        {'params': group2_params, 'lr': base_lr}\n",
        "    ], betas=(0.9, 0.95), weight_decay=0.1)\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Load checkpoint if continuing training\n",
        "    if continue_training and os.path.exists(hyperparams['checkpoint_path']):\n",
        "        try:\n",
        "            print(f\"Loading checkpoint from {hyperparams['checkpoint_path']}...\")\n",
        "            checkpoint = torch.load(hyperparams['checkpoint_path'], map_location=device)\n",
        "\n",
        "            try:\n",
        "                # Try to load model states directly\n",
        "                main_model.load_state_dict(checkpoint['main_model_state'], strict=False)\n",
        "                memory1.load_state_dict(checkpoint['memory1_state'])\n",
        "                if 'memory2_state' in checkpoint:\n",
        "                    memory2.load_state_dict(checkpoint['memory2_state'])\n",
        "                if 'optimizer_state' in checkpoint:\n",
        "                    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "                start_epoch = checkpoint.get('epoch', 0)\n",
        "                best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "                print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading checkpoint directly: {e}\")\n",
        "                print(\"Starting pre-training from scratch.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load checkpoint: {e}\")\n",
        "            print(\"Starting pre-training from scratch.\")\n",
        "    else:\n",
        "        print(\"Starting pre-training from scratch.\")\n",
        "\n",
        "    # Training setup\n",
        "    grad_clip = 1.0\n",
        "    total_steps = hyperparams['num_epochs'] * hyperparams['steps_per_epoch']\n",
        "    current_step = start_epoch * hyperparams['steps_per_epoch']\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    def get_lr(step, warmup_steps=2000, base_lr=base_lr, min_lr=1e-5):\n",
        "        # Learning rate schedule with warmup and cosine decay\n",
        "        if step < warmup_steps:\n",
        "            return base_lr * step / warmup_steps\n",
        "        decay_steps = total_steps - warmup_steps\n",
        "        step_ = step - warmup_steps\n",
        "        cosine_decay = 0.5 * (1 + math.cos(math.pi * step_ / decay_steps))\n",
        "        return min_lr + (base_lr - min_lr) * cosine_decay\n",
        "\n",
        "    print(\"Starting pre-training on COT Logic Reasoning corpus...\")\n",
        "    for epoch in range(start_epoch, hyperparams['num_epochs']):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{hyperparams['num_epochs']} ---\")\n",
        "\n",
        "        for step in range(hyperparams['steps_per_epoch']):\n",
        "            # Periodic evaluation\n",
        "            if step % hyperparams['eval_interval'] == 0:\n",
        "                losses = estimate_loss_pretrain(main_model, memory1, memory2, train_df, val_df)\n",
        "                print(f\"Step {step}, train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "                # Save best model\n",
        "                if losses['val'] < best_val_loss:\n",
        "                    best_val_loss = losses['val']\n",
        "                    torch.save({\n",
        "                        'main_model_state': main_model.state_dict(),\n",
        "                        'memory1_state': memory1.state_dict(),\n",
        "                        'memory2_state': memory2.state_dict(),\n",
        "                        'optimizer_state': optimizer.state_dict(),\n",
        "                        'epoch': epoch,\n",
        "                        'val_loss': best_val_loss\n",
        "                    }, hyperparams['checkpoint_path'].replace('.pt', '_best.pt'))\n",
        "                    print(f\"New best model saved! Val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            # Get batches for this step\n",
        "            inputs, targets = prepare_pretraining_batches_from_cot(train_df, hyperparams['block_size'])\n",
        "\n",
        "            # Zero gradients\n",
        "            if step % hyperparams['accumulation_steps'] == 0:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            B, T = inputs.shape\n",
        "            token_emb = main_model.token_embedding(inputs)\n",
        "            pos_emb = main_model.pos_embedding(torch.arange(T, device=device).unsqueeze(0))\n",
        "            combined_emb = token_emb + pos_emb\n",
        "\n",
        "            mem_out1 = memory1(combined_emb)\n",
        "            logits = main_model.forward_with_two_memory(mem_out1, memory2)\n",
        "\n",
        "            # Calculate loss\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            mask = (targets_flat != 0).float()\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
        "            masked_loss = (loss * mask).sum() / (mask.sum() + 1e-9)\n",
        "\n",
        "            # Scale loss for gradient accumulation\n",
        "            scaled_loss = masked_loss / hyperparams['accumulation_steps']\n",
        "            scaled_loss.backward()\n",
        "\n",
        "            # Check for NaN or Inf gradients\n",
        "            has_nan_inf = False\n",
        "            for param in main_model.parameters():\n",
        "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
        "                    has_nan_inf = True\n",
        "                    param.grad = torch.zeros_like(param.grad)\n",
        "\n",
        "            if has_nan_inf:\n",
        "                print(f\"NaN or Inf gradients detected and zeroed at step {step}\")\n",
        "\n",
        "            # Apply optimizer step\n",
        "            if (step + 1) % hyperparams['accumulation_steps'] == 0:\n",
        "                # Update learning rate\n",
        "                lr = get_lr(current_step)\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "\n",
        "                # Clip gradients\n",
        "                torch.nn.utils.clip_grad_norm_(main_model.parameters(), grad_clip)\n",
        "                torch.nn.utils.clip_grad_norm_(memory1.parameters(), grad_clip)\n",
        "                torch.nn.utils.clip_grad_norm_(memory2.parameters(), grad_clip)\n",
        "\n",
        "                optimizer.step()\n",
        "                current_step += 1\n",
        "\n",
        "        # Generate sample at end of epoch\n",
        "        try:\n",
        "            print(\"\\nGenerating sample text...\")\n",
        "            sample_text = generate_from_prompt(\n",
        "                main_model, memory1, memory2,\n",
        "                prompt_text=hyperparams['start_prompt'],\n",
        "                max_new_tokens=256\n",
        "            )\n",
        "            print(f\"Sample (first 200 bytes): {sample_text[:200]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating sample: {e}\")\n",
        "\n",
        "        # End of epoch checkpoint\n",
        "        torch.save({\n",
        "            'main_model_state': main_model.state_dict(),\n",
        "            'memory1_state': memory1.state_dict(),\n",
        "            'memory2_state': memory2.state_dict(),\n",
        "            'optimizer_state': optimizer.state_dict(),\n",
        "            'epoch': epoch + 1,\n",
        "            'val_loss': best_val_loss\n",
        "        }, hyperparams['checkpoint_path'])\n",
        "        print(f\"Checkpoint saved at epoch {epoch+1} to {hyperparams['checkpoint_path']}.\")\n",
        "\n",
        "    print(\"Pre-training complete!\")\n",
        "\n",
        "# ==========================================\n",
        "# 10) Script Main Entry Point\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Starting pre-training on COT corpus...\")\n",
        "    pretrain(continue_training=hyperparams['continue_training'])"
      ],
      "metadata": {
        "id": "mEBx9VukDQfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}